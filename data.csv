Title,Link,Article
Cybersecurity: A Trojan Horse in Our Digital Walls?,https://dzone.com/articles/cybersecurity-a-trojan-horse-in-our-digital-walls,"[""The rapid advancement of artificial intelligence (AI) in cybersecurity has been widely celebrated as a technological triumph. However, it's time to confront a less discussed but critical aspect: Is AI becoming more of a liability than an asset in our digital defense strategies? I talk about the unintended consequences of AI in cybersecurity in this essay, challenging the prevailing notion of AI as an unalloyed good."", 'I’ll start off with the example of deep penetration testing, a critical aspect of cybersecurity that has been utterly transformed by AI. We used to traditionally rely on formulaic methods that were confined to identifying known vulnerabilities and referencing established exploit databases. But AI? It’s changed the game entirely. AI algorithms today are capable of uncovering previously undetectable vulnerabilities by making use of advanced techniques like pattern recognition, machine learning, and anomaly detection. These systems learn from each interaction with the environment and keep adapting continuously. They can intelligently identify and exploit weaknesses that traditional methods might overlook. That’s an improvement, right?', ""Not entirely — this innovation comes with a significant caveat. The very AI systems we’ve designed to be our digital watchdogs can be repurposed by cyber attackers for malicious purposes. In such cases, AI doesn't just identify vulnerabilities; it actively crafts and executes sophisticated attack strategies. These AI-driven penetration tools, constantly learning and evolving, aren’t a concern for the distant future, by the way; they’re a current reality, with instances of such tools being utilized in cyber-attacks increasingly reported."", 'Social engineering, too, has been fundamentally transformed by AI. Remember the days when the effectiveness of social engineering relied heavily on human ingenuity – the ability to manipulate, persuade, or deceive human targets? Those days are now behind us.', 'With AI, attackers can automate and scale their deceptive tactics. AI systems now employ natural language processing and deep learning to analyze communication patterns, allowing them to mimic the linguistic style and tone of specific individuals. This can take attacks such as voice spoofing to a whole new level. These systems also integrate information from various data points — social media activity, transaction history, and even browsing patterns - to construct detailed psychological profiles of people that can predict their behaviors, preferences, and vulnerabilities.', ""Given enough data/context, these AI-powered systems can craft highly personalized messages, simulate believable interactions, and execute large-scale phishing campaigns that are meticulously tailored to each target. Each phishing attempt is no longer a generic attempt to deceive but a highly personalized message\xa0designed to resonate with the individual's unique characteristics and vulnerabilities. This specificity significantly increases the likelihood of successful deception. It's no longer a scattergun approach but a sniper's precision strike. Each employee, from the CEO to the newest intern, becomes a potential entry point for a breach, with AI algorithms orchestrating the attack."", ""Now, talking about polymorphic malware, this is where AI's influence becomes particularly alarming. It's like giving a shape-shifter an endless array of costumes, each one designed to sneak past security unnoticed. This type of malware, inherently designed to be elusive, is capable of changing its code, structure, or behavior to evade detection. And when AI, especially something as advanced as ChatGPT, gets involved, this malware gets supercharged."", 'Polymorphic malware traditionally relied on predefined algorithms to alter its code or signature at each infection or execution. Today, though, by utilizing machine learning and natural language processing capabilities,\xa0AI-enhanced malware variants can autonomously generate new code sequences or modify their execution patterns. This continuous, autonomous mutation means that the malware can adapt in real time, altering its characteristics to evade detection systems.', ""Signature-based detection systems, the basis of traditional antivirus solutions, are particularly vulnerable in this new scenario. These systems rely on identifying specific patterns or 'signatures' present in known malware variants. AI-driven polymorphic malware can bypass these detection methods by consistently changing its signature, rendering the signature-based approach less effective."", 'Similarly, behavior-based detection systems, designed to identify suspicious behavior patterns indicative of malware, also struggle against the adaptability of AI-driven polymorphic malware. These systems rely on machine learning algorithms to predict and identify malware based on behavioral patterns. However, AI-driven polymorphic malware can dynamically alter its behavior, staying one step ahead of predictive analytics and behavioral heuristics.', ""The capability of AI-driven polymorphic malware to evolve and adapt bears a scary resemblance to biological viruses that mutate to develop resistance to antibiotics. Just as these biological entities evolve to survive in changing environments and against medical interventions, AI-driven polymorphic malware continuously evolves its code and behavior to resist cybersecurity measures.What becomes increasingly clear is that AI, in the realm of cybersecurity, is a double-edged sword. For every advance in AI-driven defense, there seems to be an equal, if not greater, advance in AI-driven offense. We are in a race, but it's a race where our opponent is using the same cutting-edge tools as we are. The question then becomes: Are we inadvertently equipping our adversaries with better weapons in our quest to fortify our digital domains?""]"
The Future of Digital Products: Will AI-Assistants Replace Applications?,https://dzone.com/articles/the-future-of-digital-products-will-ai-assistants-1,"['The rapid evolution of generative AI has provoked another round of heated discussions about its effect on the future of technology—as well as our daily lives. I would like to jump in and speculate on how this may change the future of digital end-user products.\xa0', 'Here is my main hypothesis: If AI assistants like ChatGPT continue to evolve at the same pace, we will witness the end of the era of the apps as we know them.', 'In the future, I probably wouldn’t want to install a separate app on my smartphone just to order food when I can ask an assistant to order ""a sour snack to go with beer."" Why would I need a separate app to search for airline tickets and hotels when I can ask the assistant to plan my trip? Therefore, it is much more convenient to link a service provider to my assistant and interact with it in the most intuitive way, which is human dialogue.', 'It seems like digital products in the future will transform into something like AI adapters for the real world. It will provide AI assistants with access to a variety of offline services, such as ordering goods and services, making appointments at the gym or barbershop, and accessing news and entertainment content.', 'Why do I see it this way? In my opinion, ChatGPT, Siri or Alexa have strong potential to become a universal interface through which we will comfortably use a large number of services. Clearly, now it can only communicate by voice and text, but I am almost certain that in the future, it will be able to display information in an easier, better-structured way: in the form of product cards, online maps with pins, calendars, interactive widgets, visual instructions, and so on. By doing so, the market of AI assistants will offer an ultimate way of interacting with services. \xa0', 'How should the service providers react to this? Simple enough: they should integrate their solutions into these growing ecosystems. The iOS SDK allows the creation of custom commands for Siri, and Alexa developers allow the creation of voice commands for third-party apps using API. Many products can already integrate with voice assistants in order to perform such tasks as playing music, managing smart homes, and checking the news or weather. However, these use cases are pretty basic; they only involve a direct request and response. Future development of these tools can lead to a more advanced scheme:', 'Abstract request →Analysis and supplier selection → Personalised response.', 'In my opinion, one of the potential advantages of this development is that it allows service creators to directly focus on the important things, such as the quality of the product and its monetization model, instead of wasting time on less relevant features, such as a recommendation system or a product display. All of these services and products will be available through a single interface.', ""The essence of these assistants may seem similar to the trendy super-apps, only more modular, but the approach is still a bit different. A super-app offers all its features to the user at the start, while an AI assistant doesn't offer anything until it is demanded by the user."", 'This leads to another problem: how do you advertise your services if the user spends more time interacting with the assistant interface than with advertising platforms? I assume that this may be regulated by the same laws of the market as we know them—demand creates supply. And it is quite possible to imagine that every morning your assistant will offer you a paid digest with its new features based on suppliers’ advertisements.', 'The range of products that will have to adapt to the new format is quite wide, but still, there are some that are unlikely to be affected, at least not in the way I described above. These include applications where user interaction is at the center of the experience—for example, video games and other types of interactive entertainment. Indeed, this also includes any product based on an active input/output cycle.', 'We can also imagine how products that are only available online will be adapted to the new norm. Let’s take social networks as an example. I think it would be convenient to use them not only by scrolling and double-tapping but also by asking your AI-assistant things like ""How is my friend Stephanie doing?"" and getting a response of this type: ""According to the latest posts from Stephanie, she is feeling down because AI is taking away jobs. Would you like to read more?""', 'In the corporate environment, large language models are already showing great potential in terms of compiling and interpreting reports, analyzing data, and assisting with routine tasks. In fact, people who are now actively using modern generative AI are more likely to use it to improve their work efficiency than to facilitate their off-work daily routine.', 'An example of a product that partly illustrates my vision is Humane\'s recently released AI Pin, a smart pin with built-in GPT access. It has an unconventional but curious interaction interface—a combination of a laser projector and voice assistant, without third-party app support. The bot is integrated with the Tidal service that allows playing music—this is essentially the example of an ""adaptor to the real world"" concept that I mentioned above.', ""I agree that now this product seems strange and hard to use, \xa0but in my opinion, it's a good representation of my idea: the modern personal device market may soon come to its end thanks to the development of AI assistants. Interaction interfaces will grow to be more and more intuitive, such as through voice, gestures, or images. The interfaces of the services as we know them now may become less relevant. UX approaches may shift their focus from texts and buttons to data markup."", 'I have already given some examples of how this idea is being developed these days in the form of voice assistants. They have been on the market for a long time, but soon, they may become much more useful thanks to the integration of generative AI.', 'Voice assistants already allow many people to solve their routine tasks a little faster, yet truly smart assistants are still at the very early stage of their development.', ""One of the main unsolved problems is data security. Many large companies now forbid their employees to use ChatGPT in their work because of the great risk of leaks. Even though OpenAI has launched its Enterprise program, according to which they are obliged not to use the received data for model training, for regular users, this is still an acute issue. You don't really want the AI to accidentally tell everyone about your personal preferences and habits or disclose any confidential information."", 'The second problem is performance. Right now, Microsoft is saying that they have to rethink their server infrastructure because of OpenAI. Training and running large language models, especially at the GPT-4 level, requires a huge amount of computational resources. It may simply not be enough for a qualitative leap in speed and service availability. This is why offline assistants that can actually solve the data security problem are still rather limited in their capabilities.', 'The third problem is that Large Language Models (LLM) are developing too quickly. Users simply do not have time to get used to them and get to know all the possibilities. Many people still see it more as a toy. Even if the first two problems were solved tomorrow, it might take quite some time for the market to adapt to this paradigm shift and understand the real potential of new generations of AI.', ""And while all these problems are being solved, I would recommend keeping a close eye on the current trends. Sure enough, OpenAI, Anthropic, Google, and other companies seem to be leading the way in innovation, and we can't predict what it will all look like a year from now. Unfortunately, the industry giants have a monopoly on large-scale research in the area due to the huge infrastructure that is available to them."", 'However, service owners and service providers should probably start to estimate how their products may transform in the future.', 'What is your idea on the development of AI assistants? How can they affect the lives of end users and the whole digital solutions market?']"
"Revolutionizing Real-Time Alerts With AI, NATS, and Streamlit",https://dzone.com/articles/revolutionizing-real-time-alerts-with-ai-nats-and,"['Imagine you have an AI-powered personal alerting chat assistant that interacts using up-to-date data. Whether it’s a big move in the stock market that affects your investments, any significant change on your shared SharePoint documents, or discounts on Amazon you were waiting for, the application is designed to keep you informed and alert you about any significant changes based on the criteria you set in advance using your natural language.', 'In this post, we will learn how to build a full-stack event-driven weather alert chat application in Python using pretty cool tools: Streamlit, NATS, and OpenAI. The app can collect real-time weather information, understand your criteria for alerts using AI, and deliver these alerts to the user interface.', 'This piece of content and code samples can be incredibly helpful for those who love technology or those who are developers to understand how modern real-time alerting systems work with Larger Language Models (LLMs) and how to implement one.', 'You can also quickly jump on the source code hosted on our GitHub and try it yourself.', 'Let’s take a closer look at how the AI weather alert chat application works and transforms raw data into actionable alerts, keeping you one step ahead of the weather. At the core of our application lies a responsive backend implemented in Python, powered by NATS to ensure real-time data processing and message management. Integrating OpenAI’s GPT model brings a conversational AI to life, capable of understanding alerts’ nature and responding to user queries. Users can specify their alert criteria in natural language, then the GPT model will interpret them.', 'Image 1: Real-time alert app architecture', 'The journey begins with the continuous asynchronous collection of weather data from various sources in the backend. Our application now uses the api.weatherapi.com service, fetching real-time weather information every 10 seconds. This data includes temperature, humidity, precipitation, and more, covering locations worldwide. This snippet asynchronously fetches current weather data for Estonia but the app can be improved to set the location from user input dynamically:', '', 'The code segment in the main() function in the backend.py file demonstrates the integration of NATS for even-driven messaging, continuous weather monitoring, and alerting. We use the nats.py library to integrate NATS within Python code. First, we establish a connection to the NATs server running in Docker at nats://localhost:4222.', '', 'Then, we define an asynchronous message_handler function that subscribes and processes messages received on the chat subject from the NATs server. If a message starts with ""Set Alert:"" (we append it on the frontend side), it extracts and updates the user\'s alert criteria.', '', 'The backend service integrates with both external services like Weather API and Open AI Chat Completion API. If both weather data and user alert criteria are present, the app constructs a prompt for OpenAI’s GPT model to determine if the weather meets the user’s criteria. The prompt asks the AI to analyze the current weather against the user’s criteria and respond with “YES” or “NO” and a brief weather summary. Once the AI determines that the incoming weather data matches a user’s alert criteria, it crafts a personalized alert message and publishes a weather alert to the chat_response subject on the NATS server to update the frontend app with the latest changes. This message contains user-friendly notifications designed to inform and advise the user. For example, it might say, ""Heads up! Rain is expected in Estonia tomorrow. Don\'t forget to bring an umbrella!""', '', 'Let’s understand the overall communication flow between the backend and frontend.', '', '', '', 'To explore the real-time weather alert chat application in detail and try it out for yourself, please visit our GitHub repository linked earlier. The repository contains all the necessary code, detailed setup instructions, and additional documentation to help you get started. Once the setup is complete, you can start the Streamlit frontend and the Python backend. Set your weather alert criteria, and see how the system processes real-time weather data to keep you informed.', '', 'Image 2: Streamlit UI for the alert app', 'Real-time weather alert chat application demonstrated a powerful use case of NATS for real-time messaging in a distributed system, allowing for efficient communication between a user-facing frontend and a data-processing backend. However, you should consider several key steps to ensure that the information presented to the user is relevant, accurate, and actionable. In the app, we are just fetching live raw weather data and sending it straightaway to OpenAI or the front end. Sometimes you need to transform this data to filter, enrich, aggregate, or normalize it in real time before it reaches the external services. You start to think about creating a stream processing pipeline with several stages.', 'For example, not all the data fetched from the API will be relevant to every user and you can filter out unnecessary information at an initial stage. Also, data can come in various formats, especially if you’re sourcing information from multiple APIs for comprehensive alerting and you need to normalize this data. At the next stage, you enrich the data with extra context or information to the raw data to make it more useful. This could include comparing current weather conditions against historical data to identify unusual patterns or adding location-based insights using another external API, such as specific advice for weather conditions in a particular area. At later stages, you might aggregate hourly temperature data to give an average daytime temperature or to highlight the peak temperature reached during the day.', 'When it comes to transforming data, deploying, running, and scaling the app in a production environment, you might want to use dedicated frameworks in Python like GlassFlow to build sophisticated stream-processing pipelines. GlassFlow offers a fully managed serverless infrastructure for stream processing, you don’t have to think about setup, or maintenance where the app can handle large volumes of data and user requests with ease. It provides advanced state management capabilities, making it easier to track user alert criteria and other application states. Your application can scale with its user base without compromising performance.']"
Building for the Metaverse: Essential Tools and Frameworks for Developers,https://dzone.com/articles/building-for-the-metaverse-essential-tools-and-fra,"['The Metaverse is a new concept that is emerging in the digital realm. Once confined to the realms of science fiction and speculative imagination, it is now becoming a tangible reality. Thanks to the ingenuity of developers and the advancement of technology, the Metaverse is defined as a collective virtual shared space. It transcends traditional boundaries and offers limitless possibilities for exploration, interaction, and creativity.', 'As the Metaverse continues to evolve, developers find themselves at the forefront of this digital revolution, tasked with shaping its landscape and defining its potential. Armed with a diverse array of tools and frameworks, developers are pioneering the creation of immersive virtual experiences that blur the lines between reality and imagination.', ""As developers, we immerse ourselves in the intricate process of crafting virtual worlds, each line of code and pixel carefully sculpted to breathe life into the Metaverse. It's more than just coding; it's a journey of creativity and innovation, where every keystroke pushes the boundaries of what's possible. With each new development, we empower decentralized economies and reshape the landscape of digital interaction. In this dynamic ecosystem, innovation knows no bounds, and it's our passion for pushing the limits that drive us forward, shaping the future of the Metaverse and redefining the way we connect in the digital realm."", ""In this blog, we'll explore the essential tools and frameworks that empower developers to build for the Metaverse. From powerful game engines to blockchain-integrated platforms, these technologies are driving the evolution of virtual reality and shaping the landscape of the Metaverse. Join us on a journey into the heart of the digital frontier, where creativity knows no bounds and the possibilities are endless."", ""As developers dive into the Metaverse, they're stepping into a world of endless possibilities. Metaverse tools are like their trusty companions, helping them build amazing virtual experiences. These tools are like magic wands, letting developers create immersive worlds where anything is possible. From simple frameworks to cool tech, these tools are like keys that unlock the door to a whole new digital universe. So, let's embrace these tools and get ready to explore the wonders of the Metaverse together!"", 'The Oculus Developer Hub, now known as Meta Quest Developer Hub (MQDH), serves as a valuable tool for accelerating app development and measuring performance. Its redesigned user interface enhances workflow efficiency, offering features for managing files, recorded metrics, videos, images, and on-device apps. The expanded and redesigned performance analyzer tab provides developers with comprehensive insights into their applications, aiding in optimization and troubleshooting.', 'ARCore, a part of Google Play services for AR, provides developers with a software development kit (SDK) for creating augmented reality (AR) applications. By utilizing environmental understanding, light estimation, and motion-tracking technologies, ARCore enables the integration of virtual content with the real world. This allows developers to build immersive AR experiences using the camera of a smartphone or tablet, opening up new possibilities for interactive applications and games.', ""The Decentraland SDK empowers developers to participate in the creation of virtual reality-based open-world environments within the Decentraland platform. By leveraging tokens such as LAND, MANA, and Estate, developers can build applications, marketplaces, and environments. Decentraland's decentralized autonomous organizations (DAOs) enable users to govern and control their virtual worlds, fostering creativity, community engagement, and economic opportunities."", 'Mozilla Hubs is an open-source virtual reality chat room accessible through any browser or headset. It provides a platform for users to create and interact in virtual spaces, facilitating social interaction and collaboration. With features like one-click room creation and cross-platform compatibility, Mozilla Hubs promotes inclusivity and accessibility in virtual communication, making it an ideal tool for developers seeking to build immersive social experiences.', ""The VRChat SDK offers tools for building and updating content within VRChat, a social platform where users can create and share virtual worlds. With quality-of-life improvements and UI enhancements, VRChat SDK simplifies content creation and management, enabling developers to create immersive experiences and engage with a vibrant community of users within VRChat's virtual environment."", 'Blender is a free and open-source 3D creation suite that supports various aspects of content creation, including modeling, animation, rendering, and simulation. Its flexible workflow and extensive feature set make it a popular choice for creating high-quality 3D assets in the metaverse. Developers can utilize Blender to design intricate virtual environments, characters, and objects, enhancing the visual richness of their virtual experiences.', 'Adobe Creative Cloud offers a comprehensive suite of web services, resources, and applications for creative projects, including photography, video editing, graphic design, and social media. With dynamic content creation tools and resources like stock templates and tutorials, Adobe Creative Cloud supports developers in crafting compelling experiences within the metaverse. From designing immersive environments to creating engaging multimedia content, Adobe Creative Cloud provides developers with the tools they need to bring their creative vision to life.', ""In the rapidly evolving landscape of the Metaverse, developers are constantly seeking innovative tools and frameworks to build immersive virtual experiences. From open-source platforms to blockchain-integrated projects, the Metaverse offers a plethora of options for developers to explore. Let's dive into some of the notable Metaverse frameworks that are shaping the future of virtual reality development."", 'For developers who prioritize open-source principles, Webaverse offers a customizable ""metaverse engine"" for building virtual worlds. By decentralizing power from big tech companies, it promotes transparency and community-driven innovation. Developers can host Webaverse anywhere with Node.js 17 or later, providing a flexible foundation for Metaverse development.', 'HyperCube is a blockchain project designed to power the HyperVerse, an extended-reality environment similar to the Metaverse. While still in development, it offers promise for developers interested in integrating blockchain technology into Metaverse applications. By embracing decentralization, HyperCube fosters interoperability and user empowerment, offering an alternative approach to Metaverse platform development.', 'With ambitions to simplify Metaverse development, XREngine offers a comprehensive framework similar to building a website. This open-source project combines various tools to facilitate 3D world creation, communication features, and user management. As a leading open-source development framework, XREngine streamlines the development process, making Metaverse creation accessible to a wider audience of developers.', 'JanusWeb focuses on creating 3D environments within web browsers, providing an open-source framework for Metaverse programming. Leveraging technologies like Three.js and WebGL, it enables developers to build immersive virtual worlds accessible through standard web browsers. While its scope may be narrower compared to general-purpose frameworks, JanusWeb offers a solid foundation for developers focused on 3D world creation in the Metaverse.', 'Hardware plays a crucial role in Metaverse experiences, and the WebXR Device API simplifies access to virtual reality and augmented reality hardware for web applications. This standardized API provides developers with tools to seamlessly integrate hardware devices into Metaverse applications, enhancing the immersive nature of virtual experiences.', 'OMI ensures interoperability and collaboration within the Metaverse ecosystem by creating open standards for development. It offers valuable resources and APIs for developers seeking to build interconnected virtual worlds and platforms, fostering innovation and collaboration within the Metaverse community.', 'While not specifically designed for the Metaverse, Blender is a cornerstone of 3D modeling and visualization within the open-source ecosystem. With its robust suite of tools, Blender empowers developers to create and manipulate 3D assets essential for Metaverse applications. Whether crafting virtual environments or designing interactive elements, Blender serves as a fundamental platform for developers venturing into the realm of the Metaverse.', ""In conclusion, the Metaverse presents a realm of endless possibilities for developers, and the tools and frameworks available empower them to bring their creative visions to life. From Oculus Developer Hub for performance optimization to Decentraland SDK for building virtual worlds, each tool serves a unique purpose in advancing Metaverse development. Whether it's creating immersive social experiences with Mozilla Hubs or integrating blockchain technology with HyperCube, developers have a diverse array of options at their disposal. As the Metaverse continues to evolve, these tools will play a crucial role in shaping its landscape and defining its potential. So, let's embrace these tools and embark on a journey into the digital frontier of the Metaverse together!""]"
"Vector Database for LLMs, Generative AI, and Deep Learning",https://dzone.com/articles/vector-database-for-llms-generative-ai-and-deep-le,"['A vector database is a type of database specifically designed to store and manage vector data using arbitrary but related coordinates to related data. Unlike traditional databases that handle scalar data (like numbers, strings, or dates), vector databases are optimized for high-dimensional data points. But first, we have to talk about vector embeddings.', 'Vector embeddings are a method used in natural language processing (NLP) to represent words as vectors in a lower-dimensional space. This technique simplifies complex data for processing by models like Word2Vec, GloVe, or BERT. These real-world embeddings are highly complex, often with hundreds of dimensions, capturing nuanced attributes of words.', 'So, how can we benefit from vectors in fields such as AI and deep learning? Vector databases offer significant benefits to the machine learning and AI field by providing efficient and scalable solutions for storing, searching, and retrieving high-dimensional data.', 'The database uses mathematical operations, such as distance metrics, to efficiently search, retrieve, and manipulate vectors. This organization enables the database to quickly find and analyze similar or related data points by comparing the numerical values in the vectors. As a result, vector databases are well-suited for applications like similarity search, where the goal is to identify and retrieve data points that are closely related to a given query vector. This is particularly useful in applications like image recognition, natural language processing, and recommendation systems.', '', 'Initially, the process involves storing some text in the designated vector database. The received text undergoes a transformation into a vector form using the chosen AI model. Moving on, the newly created vector is then stored inside the vector database.\xa0', ""When a search prompt is issued, it's similarly converted into vectors for comparison. The system then identifies the vectors with the highest similarity and returns them. Finally, these vectors are translated back into natural language and presented to the user as search results."", ""The integration of vector databases with Large Language Models (LLMs) like GPT-4 has revolutionized the way AI systems understand and generate human language. LLMs' ability to perform deep contextual analysis of text is the result of training these models on extensive datasets, allowing them to grasp the subtleties of language, including idiomatic expressions, complex sentence structures, and even cultural nuances.\xa0"", 'These models can achieve this by converting words, sentences, and larger text segments into high-dimensional vector embeddings that represent much more than the text, encapsulating context and semantic relationships within the text and allowing LLMs to better understand more complex ideas and situations.', 'Vector databases play a critical role in managing these complex vectors. They store and index the high-dimensional data, making it possible for LLMs to efficiently retrieve and process information. This capability is particularly vital for semantic search applications, where the objective is to understand and respond to queries in natural language, providing results based on attributed similarity rather than just keyword matching.', 'LLMs use these vectors to associate words and ideas, mirroring human understanding of language. For example, LLMs can recognize synonyms, metaphors, and even cultural references, and these linguistic relationships are represented as vectors in the database. The proximity of these vectors to each other within the database can indicate the closeness of the ideas or words they represent, enabling the model to make intelligent associations and inferences. The vectors stored in these databases represent not just the literal text but the associated ideas, concepts, and contextual relationships. This arrangement allows for a more nuanced and sophisticated understanding of language.', 'Additionally, users can segment lengthy documents into several vectors and automatically store them in a vector database using a technique known as Retrieval Augmented Generation. Retrieval Augmented Generation (RAG) is a technique in the field of natural language processing and artificial intelligence that enhances the process of generating text by incorporating an external knowledge retrieval step. This approach is particularly useful for creating AI models that produce more informed, accurate, and contextually relevant responses.', 'This approach is pivotal in addressing one of the key limitations of traditional LLMs – their reliance on a fixed dataset acquired during their initial training phase, which can become outdated or lack specific details over time.', '', 'Moving on, Generative AI is a significant application of LLMs and using vector databases. Generative AI encompasses technologies like image generation, music composition, and text creation, which have seen remarkable advancements partly due to the effective use of vector databases.', 'Vector databases also play a pivotal role in enhancing the capabilities of generative AI systems by efficiently managing the complex data they require and produce.\xa0Specialized transformers are essential for converting various objects, such as images, audio, and text, into their respective comprehensive vector representations.', 'In generative AI applications similar to LLMs, the ability to categorize and retrieve content efficiently is crucial. For instance, in image generation, a vector database can store feature vectors of images. These vectors represent key characteristics of the images, such as color, texture, or style. When a generative model needs to create a new image, it can reference these vectors to find and use similar existing images as inspiration or context. This process aids in creating more accurate and contextually relevant generated content.', 'The integration of vector databases with LLMs facilitates more innovative applications, such as cross-modal AI tasks. In which two different vector entities are matched together for AI tasks. This includes tasks like converting text descriptions to images or vice versa, where understanding and translating between different types of vector representations is key.', 'Vector databases are also instrumental in handling user interaction data within generative AI systems. By encoding user preferences, behaviors, or responses as vectors, these databases allow generative models to tailor their outputs to individual users.', ""In music recommendation systems, for instance, user interactions such as played songs, skipped tracks, and time spent on each song are converted into vectors. These vectors then inform the AI about a user's musical tastes, enabling it to recommend songs that are more likely to resonate with them. As users' preferences evolve, vector databases continuously update the vector representations, allowing the AI to stay in sync with these changes. This dynamic adaptation is key to maintaining the relevance and effectiveness of personalized AI applications over time."", '', 'Vector databases represent a significant leap in data management technology, particularly in their application to AI and machine learning. By efficiently handling high-dimensional vectors, these databases have become essential in the operation and development of advanced AI systems, including LLMs, generative AI, and deep learning.', ""Their ability to store, manage, and rapidly retrieve complex data structures has not only enhanced the performance of these systems but also opened new possibilities in AI applications. From semantic search in LLMs to feature extraction in deep learning, vector databases are at the heart of modern AI's most exciting advancements. As AI continues to grow in sophistication and capability, the importance of vector databases is only set to increase, solidifying their position as a key component in the future of AI and machine learning.""]"
MLOps vs. DevOps: The Key Similarities and Differences,https://dzone.com/articles/mlops-vs-devops-the-key-similarities-and-differenc,"['DevOps has been an integral part of software development for the last 15 years. The ‘shift left’ culture, as it is popularly known, is employed across various organizations as it introduced new technologies, automation, and people systems to help shorten the software development lifecycle and provide continuous delivery of high-quality software.', '', 'With the rise of Artificial Intelligence in recent years, the structure of how enterprises are delivering and consuming AI has changed drastically with the proliferation of open-source technology. MLOps is the logical reaction to the current difficulties enterprises face putting machine learning into production.', '', 'Machine Learning Operations (MLOPs) is a primary function of Machine Learning Engineering, focused on streamlining the process of taking Machine Learning models to production, in addition to their maintenance and monitoring requirements.\xa0', 'Similar to DevOps, it’s a collaborative function that involves Data Scientists, DevOps Engineers, and other IT stakeholders. Essentially, MLOps practices allow users to version control the entire ML Development lifecycle.\xa0', '', 'MLOps shares many aspects of the DevOps paradigm. While DevOps brings a quick, continuously iterative approach to deploying applications, MLOps borrows the same principles to take Machine Learning models into production.\xa0', 'The table below provides an overview of how MLOps vs. DevOps compare in terms of; \xa0', ""It is important to note that each step of the MLOps process differs from DevOps due to its Development and Delivery requirements. With that in mind, let's discuss each of the steps above in detail."", '', 'Machine learning models are deployed as part of a broader application with multiple features and a functional UI. Building the application requires software developers, QA testers, database engineers and administrators, security experts, as well as IT administrators for environment setup.\xa0', 'While the roles highlighted apply to both concepts, MLOps requires additional skill sets, typically data scientists and machine learning engineers. These professionals are responsible for handling data extraction, creating data pipelines, and handling model training.', 'In the context of DevOps, development refers to coding new features into the application or fixing existing bugs in line with client requirements. This procedure involves multiple developers working on multiple modules of the application.\xa0', 'On the other hand, development under MLOps requires the development of robust and reusable ETL and model training pipelines. The constant development ensures the pipelines are up and running. Moreover, it involves experimentation with new data features and ML models for improved performance.', 'Testing is a vital part of both DevOps and MLOps. For DevOps, the testing phase evaluates the quality of the code to check if it will hold up in production. As such, DevOps involves extensive unit and integration tests and evaluates each module to check if they behave as expected.\xa0', ""These tests are also present in MLOps, along with data quality checks and model performance evaluation. Data checks evaluate data models for unexpected schema changes that may break the pipeline, while the latter evaluates the new model's performance compared to the previous ones."", 'The deployment phase involves pushing the necessary changes to the production environment. For DevOps, this means packaging the code and all its necessary configurations and dependencies and transferring them to production servers.', 'MLOps deployments involve setting up Continuous Integration (CI), Continuous Deployment (CD), and Continuous Training (CT) pipelines. CT pipelines collect new data, apply transformations, and re-train models. These new models are deployed using the CI/CD pipeline structures.', ""The production phase for both paradigms involves health checks and application monitoring. DevOps monitors applications for unexpected errors and crashes. It also monitors the server's health, checking resource utilization to prevent downtime."", 'In contrast, MLOps monitoring involves monitoring model performance to check if it complies with production standards. Furthermore, it also monitors data pipelines for any failures.', '', 'With all their benefits and advantages, MLOps procedures also face a set of unique challenges. Many of these challenges are related to data and model administration.', 'With each iteration of the training and deployment pipeline, the previous versions of the pipeline components are to be efficiently maintained.', 'Looking at the bigger picture, it is clear that MLOps is not an entirely novel concept. Instead, it is the extension of the original DevOps paradigm. It adds additional steps and functionality to accommodate the principles of data science. For this reason, the new MLOps culture benefits data scientists by streamlining the creation of data pipelines, feature engineering, model training, and deployment.', '', 'The DevOps culture is designed to automate software product development, testing, and deployment. However, conventional DevOps does not cater to data science practices, which necessitated the conception of the MLOps paradigm.', 'MLOps borrows aspects from DevOps but includes several elements to automate data science and machine learning projects. It includes a broader skill set, such as data engineers, data scientists, and machine learning engineers.', 'Furthermore, while DevOps integrates software applications, MLOps implements quality checks and tests for data pipelines and machine learning model training and deployment.\xa0', 'These implementations enable organizations to construct reliable data infrastructures and apply continuous implementation, deployment, and training for machine learning models.']"
Decoding Large Language Models and How They Work,https://dzone.com/articles/decoding-large-language-models-and-how-they-work,"[""The evolution of natural language processing with Large Language Models (LLMs) like ChatGPT and GPT-4 marks a significant milestone, with these models demonstrating near-human comprehension in text-based tasks. Moving beyond this, OpenAI's introduction of Large Multimodal Models (LMMs) represents a notable shift, enabling these models to process both images and textual data. This article will focus on the core text interpretation techniques of LLMs — tokenization and embedding — and their adaptation in multimodal contexts, signaling an AI future that transcends text to encompass a broader range of sensory inputs."", 'Turning raw text into a format that Large Language Models like GPT-4 can understand involves a series of complex and connected steps. Each of these processes — tokenization, token embeddings, and the use of transformer architecture — plays a critical role in how these models understand and generate human language.', 'The first step in processing language data for LLMs is tokenization. It is the process where text is segmented into smaller units, known as tokens. This step is critical in breaking down complex language structures into elements that a machine-learning model can process and understand.', 'Tokenization can take various forms, each with its unique advantages and challenges.', 'Character-level tokenization is the simplest form, where text is divided into individual characters. Its straightforward nature, however, leads to longer sequences of tokens, which can be less efficient for processing. On the other end of the spectrum is word-level tokenization, which splits text into words. This method is intuitive but can struggle with large vocabularies and often falters when encountering new or unknown words.', 'A more balanced approach is found in subword tokenization, exemplified by Byte Pair Encoding (BPE). BPE begins with a foundational vocabulary consisting of all unique characters in a training corpus, ensuring every word can be broken down into these basic units. The core of BPE lies in its method of frequency analysis and pair merging. It iteratively scans the corpus to identify the most frequently occurring pairs of characters or tokens. These pairs are then merged to form new tokens. This step is crucial as it enables the model to recognize and consolidate common pairings, treating them as single units in subsequent processing. The process continues, with each iteration merging the next most frequent pair.', 'As the vocabulary evolves with each iteration, new tokens (merged pairs) are added. The process repeats until the vocabulary reaches a predefined size or the desired level of granularity. The final vocabulary size strikes a balance between being detailed enough to represent complex words and manageable enough to be processed efficiently.\xa0', 'When tokenizing new text, the algorithm employs this refined vocabulary. It starts by searching for the largest possible tokens in the vocabulary. If no match is found, it breaks the text down into smaller units, continuing until it finds suitable matches. This method ensures that common words and phrases are tokenized into fewer, larger tokens, enhancing processing speed, while less common phrases are further broken down for accuracy.', ""BPE's efficiency in representing common phrases, its capability in handling rare and unknown words through subword units, and its balanced approach make it particularly effective for processing languages with large vocabularies or intricate word formations. This method exemplifies a sophisticated approach to preparing data for processing by LLMs, contributing significantly to their understanding of human language."", 'I’ll walk through an example of how BPE works, along with some code after, to make this concept more concrete.', 'Let\'s use the sentence ""This is an example"" as an example. Initially, BPE breaks down the sentence into its basic units. Rather than whole words, it starts with characters or small groups of characters. Thus, ""This is an example"" would first be tokenized into each individual character like \'T,\' \'h,\' \'i,\' \'s,\' and so on.', ""BPE then analyzes the frequency of character pairs in the training data and begins merging the most common pairs. If, for instance, 'Th,' 'is, '' an', ' ex,' and 'ample' are frequent pairs in the training data, BPE will merge these into single tokens. This process is iteratively applied, resulting in the sentence being represented by fewer, larger tokens such as 'Th,' 'is,' 'an,' 'ex,' and 'ample.'"", 'This transformation of the sentence from a sequence of individual characters to larger, more meaningful units reduces the total number of tokens needed to represent the text, maintaining efficiency without losing meaning. Additionally, BPE\'s methodology is adept at handling new or rare words. For a word not seen during training, like ""exemplary,"" BPE can break it down into known subwords or characters, such as \'ex,\' \'am,\' \'pl,\' \'ar,\' and \'y.\' This capability makes BPE particularly effective in dealing with unknown words by approximating them using its existing vocabulary of subwords.', 'I will also show you what a simplified representation of BPE would look like in code.', 'This segment prepares the initial vocabulary from a sample text corpus. Each word is split into characters, with a special end-of-word token </w> appended. This token helps distinguish between words that end in the same characters but are different words.', '', 'These functions are critical for BPE operations. They find the common pairs in the training data and merge them, as we discussed in the example above.', '', 'This segment performs the core of BPE: iteratively merging the most frequent pairs of characters in the vocabulary.', '', 'This final segment demonstrates how to use the BPE vocabulary to tokenize a new sentence.', '', 'Together, these code segments demonstrate the creation of a BPE vocabulary from a corpus and its application in tokenizing new text.', 'Tokenization is more than just a preliminary step; it lays the groundwork for deeper language comprehension. By converting text into tokens, we prepare the data for the subsequent phase of embedding, where the true semantic processing begins.', 'Once tokenized, the next step for the text is token embeddings, a process that captures the semantic meanings of words based on context. The process involves initializing embeddings, training the model to adjust these embeddings, and ensuring that semantically similar tokens are close in the embedding space.', ""Each token (word or subword) in the model's vocabulary is associated with an embedding, which is a high-dimensional vector. These embeddings are typically initialized randomly. This randomness provides a starting point for the training process to adjust and refine these vectors."", 'During training, the model is exposed to large volumes of text data. It learns to adjust the embeddings based on the context in which each token appears. This learning process involves backpropagation and optimization algorithms (like Stochastic Gradient Descent or Adam) to iteratively adjust the embeddings to minimize the prediction error of the model.', 'The model aims to adjust the embeddings so that tokens used in similar contexts have embeddings that are close to each other in the vector space. This closeness is typically measured using cosine similarity. This process creates a semantic map where words with similar meanings or usage are located near each other in the high-dimensional space.', ""I'll walk through the process of initializing token embeddings and simulating a basic training process to adjust these embeddings using a simplified example."", '', 'Token embeddings provide Large Language Models (LLMs) with a deep understanding of language context and meaning. Transformers build on this by processing these embeddings in parallel, using self-attention mechanisms to discern intricate word relationships. This synergy enables LLMs to interpret and generate text with a nuanced comprehension of language semantics and structure.', 'Transformers are designed with a layered architecture, where each layer contributes to processing the input data. The core components of each layer are the self-attention mechanism and a feed-forward neural network. What sets Transformers apart from earlier models like RNNs and LSTMs is their ability to process all tokens in an input sequence simultaneously. This parallel processing approach not only boosts computational efficiency but also enables the model to capture complex dependencies and relationships within the data more effectively.', 'At the heart of each Transformer layer is the self-attention mechanism. This mechanism computes what are known as attention scores for each token in the input sequence. These scores are a measure of how much focus or ""attention"" the model should allocate to other tokens in the sequence when processing a particular token.', 'The self-attention mechanism allows the Transformer to dynamically adjust the influence of each token in the sequence based on the entire sequence itself. This is crucial for understanding context, as it enables the model to interpret each token not in isolation but in relation to others.', 'Moreover, Transformers employ what is termed as multi-head attention. This means that the self-attention process is replicated multiple times in parallel within each layer. Each replication, or ""head,"" can potentially focus on different aspects of the token relationships, such as syntactic or semantic connections. By doing so, the model can capture a richer and more nuanced understanding of the text.', 'A unique challenge in the design of Transformers is their lack of inherent sequence processing capability – a feature that sequential models like RNNs inherently possess. To address this, Transformers utilize positional encodings. These are added to the token embeddings to provide the model with information about the position of each token within the sequence.', 'The positional encodings are often generated using sinusoidal functions. This method ensures that each position in the sequence receives a unique encoding, allowing the model to differentiate between tokens based on their order in the sequence. This positional information is crucial for the model to understand the flow and structure of language.', 'In a Transformer model, each layer processes the input sequence, progressively transforming and refining the representation of each token. This transformation is informed by the context provided by the self-attention mechanism and the subsequent operations of the feed-forward network within the layer.', 'As the input data passes through successive layers of the Transformer, the representations of the tokens become increasingly refined and enriched with contextual information. This layer-by-layer processing allows for a sophisticated understanding and generation of language, enabling the model to handle complex language tasks with remarkable efficiency and effectiveness.', ""Below is a simplified Python code example to illustrate a basic version of the Transformer's self-attention mechanism.\xa0"", '', 'This code offers a basic glimpse into how self-attention in Transformers operates. The real-world Transformer models are much more complex, with additional components like layer normalization, residual connections, and a feed-forward network in each layer, all of which are critical for their advanced performance.', 'Bridging from the text-focused realm of LLMs to the more diverse world of Large Multimodal Models (LMMs), we see an expansion in capabilities. While LLMs excel in interpreting and generating text, LMMs extend this proficiency to include other types of data, notably images. This transition marks a significant evolution in model design and functionality. LMMs integrate the text-processing power of Transformers with advanced neural network architectures capable of handling visual data, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). The synergy of these technologies in LMMs enables them to process and understand a combination of data types, like text and images.', 'In LMMs, the goal is to create a space where feature vectors from images and token embeddings from text can coexist and be processed together. This coexistence allows the model to perform operations that consider both textual and visual information, enabling a more comprehensive understanding of multimodal content. Unified representations in LMMs enable a range of applications. For example, they can generate descriptive captions for images or answer questions that require insights from both an image and accompanying text. This capability is particularly useful in scenarios like content moderation, where understanding the context of both text and images is crucial.', 'Cross-modal attention mechanisms in LMMs allow the model to focus on specific parts of one modality based on the context provided by another. For instance, when processing a text-image pair, the model might focus on specific regions of the image that are directly relevant to the textual content.', 'This cross-modal attention is pivotal for tasks that require an integrated understanding of both textual and visual information. It allows LMMs to perform sophisticated analysis, like interpreting the sentiment of an image based on its visual elements and accompanying text or providing detailed explanations of visual content.', 'I’ll walk through a Python code example to illustrate how a model might process an image and text together. This example is highly simplified and does not represent the complexity of actual LMMs.', '', 'The remarkable pace of development in Large Language Models (LLMs) and Large Multimodal Models (LMMs) paves the way for a future brimming with unprecedented possibilities. As we stand at the cusp of this transformative era, the potential of LLMs and LMMs to revolutionize various sectors — from education to industry — is both immense and inspiring.']"
"AI and Microservice Architecture, A Perfect Match?",https://dzone.com/articles/ai-and-microservice-architecture-a-perfect-match,"['In the realm of modern software development and IT infrastructure, the amalgamation of Artificial Intelligence (AI) and Microservice Architecture has sparked a revolution, promising a new era of scalability, flexibility, and efficiency. This blog delves into the synergistic relationship between AI and microservices, exploring whether they indeed constitute a perfect match for businesses and developers looking to harness the full potential of both worlds.', 'Microservice architecture, characterized by its design principle of breaking down applications into smaller, independently deployable services, has gained immense popularity for its ability to enhance scalability, facilitate continuous deployment, and improve fault isolation. Unlike monolithic architectures, microservices allow teams to deploy updates for specific functions without affecting the entire system, making it an ideal approach for dynamic and evolving applications.', 'The integration of AI into software systems introduces a new layer of complexity and capability. AI algorithms require vast amounts of data, substantial computing power, and sophisticated data processing pipelines to train and deploy models effectively. As AI continues to evolve, the need for architectures that can support the agility and scalability required by AI workloads becomes increasingly apparent.', 'One of the most compelling arguments for the compatibility of AI and microservice architecture lies in their mutual emphasis on scalability. Microservices allow systems to scale components independently, while AI applications often need to scale rapidly based on the computational demands of model training and inference. This alignment makes microservices an ideal architectural choice for deploying AI models, as it provides the flexibility to allocate resources efficiently and scale AI services as needed.', 'The microservices architecture promotes agility and faster innovation cycles, which are crucial for AI development. It enables teams to update AI models and deploy new features without overhauling the entire system, reducing time-to-market and facilitating continuous improvement. This iterative approach aligns with the experimental nature of AI, where models are constantly refined and updated based on new data and insights.', 'Microservices thrive on the principle of decoupling, where services operate independently, communicating through well-defined APIs. This decoupling is particularly beneficial for AI systems, allowing data scientists and AI developers to focus on optimizing models and algorithms without being constrained by the dependencies and complexities of a monolithic architecture. Each microservice can be dedicated to a specific AI function, such as data ingestion, preprocessing, model training, and inference, streamlining the development and maintenance processes.', ""The global artificial intelligence market, valued at USD 93.5 billion in 2021, is expected to witness a compound annual growth rate (CAGR) of 38.1% through to 2030 (Grand View Research). Concurrently, microservices have seen widespread adoption, with over 61% of organizations utilizing them in more than half of their new applications (O'Reilly, 2020).\xa0"", 'Despite the synergies, integrating AI with microservice architectures is not without challenges. Managing the complexity of distributed systems, ensuring consistent data management and integrity across services, and maintaining the performance of AI models in a microservice environment require careful planning and execution. Additionally, organizations must invest in robust infrastructure and tooling to monitor, manage, and secure microservices, especially when dealing with sensitive data and mission-critical AI applications.', 'Several industries are already reaping the benefits of combining AI with microservices. In finance, microservices are used to deploy AI-driven fraud detection algorithms that analyze transactions in real time. In healthcare, microservice architectures support AI-powered diagnostic tools, allowing rapid updates and scaling as new data becomes available. Retail and e-commerce platforms leverage AI and microservices for personalized recommendation engines, enhancing customer experiences while maintaining the agility to introduce new features swiftly.', 'So, are AI and microservice architecture a perfect match? The answer leans towards a resounding yes, with the caveat that success hinges on strategic implementation and ongoing management. The combination offers a powerful paradigm for building flexible, scalable, and innovative AI applications, provided that organizations navigate the challenges with foresight and invest in the necessary infrastructure and expertise. As technology evolves, the fusion of AI and microservices stands as a beacon for the future of software development, offering a roadmap for creating intelligent, adaptable, and resilient systems.']"
How To Deploy Machine Learning Models Using Amazon SageMaker,https://dzone.com/articles/deploy-machine-learning-models-using-amazon-sagemaker,"['Machine learning models have become an integral part of modern business applications. The increasing demand for machine learning solutions has led to a significant increase in the number of tools and platforms. These tools and platforms support developers in the training and deploying of machine learning models. Amazon SageMaker has gained popularity among data scientists and developers for its ease of use, scalability, and security.', 'So, what is Amazon SageMaker? The Amazon SageMaker is a managed machine learning platform. It provides data scientists and developers with the essential resources and tools to produce, train, and deploy machine learning models on a massive scale.', 'Scalable ML model deployment is essential for organizations dealing with massive amounts of data. By leveraging cloud-based solutions like Amazon SageMaker, businesses can deploy and scale their models efficiently and effectively.', 'SageMaker allows you to build and train models through popular machine-learning frameworks like TensorFlow, PyTorch, and Apache MXNet. SageMaker also offers pre-built algorithms for common use cases like image classification and natural language processing. In this blog post, we will discuss how to deploy ML models using Amazon SageMaker.', 'The global machine learning market is expected to grow from $21.17 billion in 2022 to $209.91 billion by 2029, at a CAGR of 38.8%.-Fortune Business Insights', 'Machine learning has become an important part of many businesses today. However, deploying machine learning models can be a challenging task, especially for scaling and managing the models. This is where Amazon SageMaker comes into the picture. Amazon SageMaker simplifies the machine learning development process by providing an integrated environment for building, training, and deploying machine learning models. Before we move on to the steps, you may want to read the top 10 reasons why SageMaker is great for ML.', 'The first step in deploying a machine learning model is to train and evaluate the model. Amazon SageMaker provides a Jupyter Notebook environment that can develop and test your machine-learning algorithms. You can use this environment to create and run your training and evaluation code. After training your model, you can save the model artifacts to Amazon S3.', 'The next step is to create a SageMaker model once you finish training and evaluating your machine-learning model. A SageMaker model is a Docker container that contains your ML model. Specify the location of your model artifacts in Amazon S3, the name of your Docker container, and the code required to load your model to generate a SageMaker model.', 'After you have created a SageMaker model, the next step is to create an endpoint configuration. An endpoint configuration is a setup that outlines the number and type of instances required to host your endpoint. You can create an endpoint configuration using the Amazon SageMaker console or the Amazon SageMaker API.', 'The next step is to deploy the model. You can deploy your model by creating an endpoint using the endpoint configuration that you created in the previous step. Amazon SageMaker provides a fully managed infrastructure for hosting your endpoint, which includes automatic scaling and load balancing.', 'AWS ML model deployment allows organizations to leverage cloud-based solutions for deploying machine learning models at scale. With Amazon SageMaker, developers can easily create, train, and deploy models on AWS infrastructure.', 'You can perform ML model monitoring using Amazon CloudWatch. It provides metrics such as latency and request count. You can also use this information to optimize your endpoint performance. Many organizations have already deployed Amazon SageMaker to perform A/B testing and compare the performance of different machine learning models.', 'Monitoring machine learning models is crucial for ensuring that they continue to perform accurately over time. Amazon SageMaker provides built-in ML model monitoring capabilities, allowing developers to identify and fix potential issues before they cause significant problems.', 'You can update the endpoint by creating a new endpoint configuration and deploying a new model. You can also delete the endpoint using the Amazon SageMaker console or the Amazon SageMaker API.', 'Deploying machine learning models using Amazon SageMaker has several advantages. Firstly, SageMaker provides a user-friendly interface and pre-built algorithms, making it easy to build, train and deploy models. In addition, SageMaker can adjust to changes in workload and deal with intricate models and huge amounts of data. Moreover, you can reduce infrastructure costs by adopting its pricing model, which is based on usage.', 'SageMaker integrates seamlessly with other AWS services, enhancing its functionality. It offers security features such as encryption and access controls to protect data and models. Lastly, SageMaker provides organizations with the ability to tailor their machine-learning pipelines using their own frameworks and algorithms. It also offers many deployment alternatives, making it adaptable for different applications.', 'Softweb is one of the leading technology consulting and development companies that focuses on delivering innovative solutions to assist businesses in harnessing the power of AI and machine learning. We offer multiple services to support businesses in implementing machine learning models using Amazon SageMaker, including data exploration and preparation, model development, and deployment.', 'Softweb’s team of machine learning engineers, data scientists, and software developers collaborate with clients to ensure that their models are optimized for accuracy and performance and deployed securely and efficiently. We have significant experience in creating and deploying machine learning models with Amazon SageMaker. We assist businesses in accomplishing their AI objectives and generating significant business results.', 'Amazon SageMaker provides a fully managed machine learning service that simplifies deploying machine learning models. In this blog post, we have looked at the steps involved in deploying machine learning models using Amazon SageMaker. These steps include training and evaluating your model, creating a SageMaker model, creating an endpoint configuration, deploying the model, monitoring and maintaining the endpoint, and updating or deleting the endpoint.', 'If you plan to deploy ML models using SageMaker, it is advisable to get help from an AWS SageMaker consulting services provider to ensure a smooth and error-free deployment of your ML models. For more information, please talk to our experts.']"
Prompt Engineering Tutorial for AI/ML Engineers,https://dzone.com/articles/prompt-engineering-tutorial-for-aiml-engineers,"[""The generative AI revolution has made significant progress in the past year, mostly in the release of Large Language Models (LLMs). It is true that generative AI is here to stay and has a great future in the world of software engineering. While models work amazingly well and produce advanced outputs, we can also influence models to produce the outputs we want. It's an art to make language models work to produce results/outputs as expected — and this is where prompt engineering comes into play. Prompts play a vital role in talking with language models. In this article, we’ll take a deeper dive into everything you need to know about prompt engineering.\xa0"", '', 'Prompt engineering is the first step toward talking with LLMs. Essentially, it’s the process of crafting meaningful instructions to generative AI models so they can produce better results and responses. This is done by carefully choosing the words and adding more context. An example would be training a puppy with positive reinforcement, using rewards and treats for obedience. Usually, large language models produce large amounts of data that can be biased, hallucinated, or fake — all of which can be reduced with prompt engineering.\xa0', 'Prompt engineering involves understanding the capabilities of LLMs and crafting prompts that effectively communicate your goals. By using a mix of prompt techniques, we can tap into an endless array of possibilities — from generating news articles that feel crafted by hand to writing poems that emulate your desired tone and style. Let’s dive deep into these techniques and understand how different prompt techniques work.', '', ""You provide a prompt directly to the LLM without any additional examples or information. This is best suited for general tasks where you trust the LLM's ability to generate creative output based on its understanding of language and concepts."", 'You provide one example of the desired output along with the prompt. This is best suited for tasks where you want to guide the LLM toward a specific style, tone, or topic.', 'You provide a few (usually two to four-) examples of the desired output along with the prompt. This is best suited for tasks where you need to ensure consistency and accuracy, like generating text in a specific format or domain.', 'This focuses on breaking down complex tasks into manageable steps, fostering reasoning and logic; think of dissecting a math problem into bite-sized instructions for the LLM.', 'This involves providing relevant background information to enhance accuracy and coherence; imagine enriching a historical fiction prompt with detailed research on the era.', 'This process involves fine-tuning the overall LLM behavior and blending multiple prompting styles; think of meta-prompts as overarching principles while combinations leverage different techniques simultaneously.', 'Lastly, this process integrates human feedback and iteratively refining prompts for optimal results; picture a collaborative dance between humans and LLM, with each iteration enhancing the outcome.', 'These techniques are just a glimpse into the vast toolbox of prompt engineering. Remember, experimentation is key — tailor your prompts to your specific goals and embrace the iterative process.', ""Best practices in prompt engineering involve understanding the capabilities and limitations of the model, crafting clear and concise prompts, and iteratively testing and refining prompts based on the model's responses. Whether you're an AI developer, researcher, or enthusiast, these best practices will enhance your interactions with advanced language technologies, leading to more accurate and efficient outcomes."", 'Be clear about your desired outcome with specific instructions, desired format, and output length. Think of it as providing detailed directions to a friend, not just pointing in a general direction.', 'Give the model relevant context to understand your request. Provide background information, relevant examples, or desired style and tone. Think of it as setting the scene for your desired output.', 'Show the model what you want by providing examples of desired output, helping narrow down the possibilities, and guiding the model toward your vision. Think of it as showing your friend pictures of the destination — instead of just giving them the address.', 'Choose clear, direct, and unambiguous language. Avoid slang, metaphors, or overly complex vocabulary. Remember, the model interprets literally, so think of it as speaking plainly and clearly to ensure understanding.', ""Don't expect perfect results in one try. Be prepared to revise your prompts, change context cues, and try different examples. Think of it as fine-tuning the recipe until you get the perfect dish."", ""Understand the capabilities and limitations of the specific model you're using. For example, some models are better at factual tasks, while others excel at creative writing — so choose the right tool for the job."", ""Be mindful of potential biases in both your prompts and the model's output. Avoid discriminatory language or stereotypes, and use prompts that promote inclusivity and ethical considerations."", 'I hope these best practices help you craft effective prompts and unlock the full potential of large language models!', 'You need SingleStore Notebooks to carry out this tutorial.\xa0', 'The first time you sign up, you will receive $600 in free computing resources.', 'Now, click on the Notebook icon to start.\xa0', '', 'Create a new Notebook and name it as ‘Prompt-Engineering-Tutorial’', '', 'Let’s get started with understanding how prompt engineering works practically by running our code snippets inside this Notebook.', 'We will use the LangChain framework to create prompt templates and use them in our example tutorial. New to LangChain? Check out this beginner’s guide for everything you need to know. \xa0', 'LangChain provides tooling to create and work with prompt templates.', 'First things first, install the necessary dependencies and libraries — we need OpenAI and LangChain.', '', 'Next, provide the OpenAI API Key.\xa0', 'Create a prompt template as specified in the LangChain documentation.\xa0', '', 'Set up the environment and create the LLMChain.', '', 'Run the LLMChain with zero-shot prompt input. Here, my input is ‘large language models.’\xa0', '', 'This is how a few-shot prompt template looks.\xa0', '', 'The complete Notebook code is available in this GitHub repository.\xa0', 'Understanding some common skills required to become a prompt engineer is very important. As the bridge between human intentions and artificial intelligence responses, the role demands a unique blend of technical and soft skills — we have listed six here you should consider creating meaningful and effective prompts.\xa0', 'Essential for understanding user needs and working with internal teams, effective communication and collaborative skills enable prompt engineers to articulate complex ideas and integrate input from various stakeholders. They foster a productive environment, ensuring the development of prompts aligns with user intent and technical capabilities.', 'These skills are vital for generating innovative and effective prompts. Creative thinking allows for the exploration of novel ideas and approaches, while critical thinking ensures that prompts are logical, ethical, and meet the desired outcome. This blend of creativity and scrutiny is crucial for crafting quality prompts that engage users and perform as intended.', 'Understanding the fundamentals of artificial intelligence and natural language processing is crucial for a prompt engineer. This knowledge helps in grasping how AI models interpret and generate language, enabling the creation of prompts that effectively communicate with the AI to produce accurate and relevant responses.', ""Mastery of various writing styles is important for prompt engineering since different scenarios require different types of communication. Whether it's concise instructional writing, creative storytelling, or technical documentation, the ability to adapt writing style to fit the context is key in crafting effective prompts."", 'A strong foundation in programming — particularly in languages like Python — is beneficial for prompt engineers. Programming skills allow for the automation of tasks, manipulation of data, and even customizing AI models. This technical proficiency can significantly enhance the efficiency and capability of prompt development.', 'Understanding data analysis is important for prompt engineers since it allows them to interpret user interactions and model performance. This knowledge helps in refining prompts based on empirical evidence, ensuring they are optimized for both user engagement and accuracy. Data analysis skills are crucial for continuous improvement of prompt effectiveness.', 'Prompt engineering is a critical and evolving field that enables more effective interactions with AI models. By understanding its core principles, exploring various techniques, and adhering to best practices, users can craft prompts that significantly enhance the performance and relevance of AI responses. The simple tutorial provided underscores the practicality of prompt engineering, offering a hands-on approach to mastering this art. As gen AI continues to advance, the importance of skillful prompt engineering grows —promising a future of more intuitive, efficient, and powerful human-AI collaborations.']"
NIST AI Risk Management Framework: Developer’s Handbook,https://dzone.com/articles/exploring-the-nist-ai-risk-management-framework-a,"['The NIST AI RMF (National Institute of Standards and Technology Artificial Intelligence Risk Management Framework) provides a structured framework for identifying, assessing, and mitigating risks associated with artificial intelligence technologies, addressing complex challenges such as algorithmic bias, data privacy, and ethical considerations, thus helping organizations ensure the security, reliability, and ethical use of AI systems. \xa0', 'AI risks differ from traditional software risks in several key ways:', 'Effectively managing AI risks requires specialized knowledge, tools, and frameworks tailored to the unique characteristics of AI technologies and their potential impact on individuals, organizations, and society as a whole.', 'The AI RMF refers to an AI system as an engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments. The AI RMF helps organizations effectively identify, assess, mitigate, and monitor risks associated with AI technologies throughout the lifecycle. It addresses various challenges, like data quality issues, model bias, adversarial attacks, algorithmic transparency, and ethical considerations. Key considerations include:', 'Following are the essential functions within the NIST AI RMF that help organizations effectively identify, assess, mitigate, and monitor risks associated with AI technologies.', '', 'Image courtesy of NIST AI RMF Playbook', 'Governance in the NIST AI RMF refers to the establishment of policies, processes, structures, and mechanisms to ensure effective oversight, accountability, and decision-making related to AI risk management. This includes defining roles and responsibilities, setting risk tolerance levels, establishing policies and procedures, and ensuring compliance with regulatory requirements and organizational objectives. Governance ensures that AI risk management activities are aligned with organizational priorities, stakeholder expectations, and ethical standards.', ""Mapping in the NIST AI RMF involves identifying and categorizing AI-related risks, threats, vulnerabilities, and controls within the context of the organization's AI ecosystem. This includes mapping AI system components, interfaces, data flows, dependencies, and associated risks to understand the broader risk landscape. Mapping helps organizations visualize and prioritize AI-related risks, enabling them to develop targeted risk management strategies and allocate resources effectively. It may also involve mapping AI risks to established frameworks, standards, or regulations to ensure comprehensive coverage and compliance."", 'Measurement in the NIST AI RMF involves assessing and quantifying AI-related risks, controls, and performance metrics to evaluate the effectiveness of risk management efforts. This includes conducting risk assessments, control evaluations, and performance monitoring activities to measure the impact of AI risks on organizational objectives and stakeholder interests. Measurement helps organizations identify areas for improvement, track progress over time, and demonstrate the effectiveness of AI risk management practices to stakeholders. It may also involve benchmarking against industry standards or best practices to identify areas for improvement and drive continuous improvement.', ""Management in the NIST AI RMF refers to the implementation of risk management strategies, controls, and mitigation measures to address identified AI-related risks effectively. This includes implementing selected controls, developing risk treatment plans, and monitoring AI systems' security posture and performance. Management activities involve coordinating cross-functional teams, communicating with stakeholders, and adapting risk management practices based on changing risk environments. Effective risk management helps organizations minimize the impact of AI risks on organizational objectives, stakeholders, and operations while maximizing the benefits of AI technologies."", 'The NIST AI RMF consists of two primary components:', ""This part includes introductory materials, background information, and context-setting elements that provide an overview of the framework's purpose, scope, and objectives. It may include definitions, principles, and guiding principles relevant to managing risks associated with artificial intelligence (AI) technologies."", 'This part comprises the core set of processes, activities, and tasks necessary for managing AI-related risks, along with customizable profiles that organizations can tailor to their specific needs and requirements. The core provides a foundation for risk management, while profiles allow organizations to adapt the framework to their unique circumstances, addressing industry-specific challenges, regulatory requirements, and organizational priorities.', '', '', '', ""The NIST AI Risk Management Framework offers a comprehensive approach to addressing the complex challenges associated with managing risks in artificial intelligence (AI) technologies. Through its foundational information and core components, the framework provides organizations with a structured and adaptable methodology for identifying, assessing, mitigating, and monitoring risks throughout the AI lifecycle. By leveraging the principles and guidelines outlined in the framework, organizations can enhance the security, reliability, and ethical use of AI systems while ensuring compliance with regulatory requirements and stakeholder expectations. However, it's essential to recognize that effectively managing AI-related risks requires ongoing diligence, collaboration, and adaptation to evolving technological and regulatory landscapes. By embracing the NIST AI RMF as a guiding framework, organizations can navigate the complexities of AI risk management with confidence and responsibility, ultimately fostering trust and innovation in the responsible deployment of AI technologies.""]"
Art of Code: Elegant Solutions in Software Development,https://dzone.com/articles/art-of-code-elegant-solutions-in-software-developm,"['In the vast landscape of software development, where lines of code intertwine to create the digital foundations of our modern world, there exists a subtle yet profound artistry. Beyond the mere functionality lies a realm where elegance, efficiency, and creativity converge to sculpt solutions that not only solve problems but do so with grace and ingenuity. Welcome to the realm where coding transcends mere technicality and transforms into an art form.', 'At its core, coding is a form of expression. Just as a painter wields brushes and colors to create masterpieces on canvas, a programmer utilizes algorithms and syntax to breathe life into lines of code. However, the artistry of coding extends beyond the act of typing characters into an editor. It manifests in the way problems are approached, solutions are devised, and complexities are elegantly untangled.', 'In the art of coding, simplicity reigns supreme. Like a minimalist painting that speaks volumes with a few well-placed strokes, elegant code achieves its purpose with clarity and conciseness. This principle, often encapsulated in the famous quote by Antoine de Saint-Exupéry, ""Perfection is achieved not when there is nothing more to add, but when there is nothing left to take away,"" underscores the importance of simplicity in crafting code that is not only functional but also beautiful.', ""Central to the art of coding is the mastery of algorithms. Just as a composer weaves melodies and harmonies to create a symphony, a programmer orchestrates algorithms to solve complex problems efficiently. Whether it's sorting arrays, searching graphs, or optimizing routes, the choice and implementation of algorithms can transform mundane tasks into feats of computational prowess."", 'In the world of software development, design patterns are the choreography that guides the dance of code. These reusable solutions to common problems provide a framework for structuring code in a way that is both flexible and maintainable. From the classic Gang of Four patterns to modern architectural paradigms, the judicious application of design patterns empowers developers to craft code that is not only functional but also resilient to change.', ""Just as a painter explores different styles and techniques to evoke diverse emotions, a programmer delves into various programming paradigms to tackle different types of problems. Whether it's the procedural elegance of imperative programming, the declarative clarity of functional programming, or the dynamic expressiveness of object-oriented programming, each paradigm offers a unique perspective on how to approach and solve challenges in software development."", 'Like any form of art, mastering the art of coding is a journey fraught with challenges and triumphs. It requires patience, perseverance, and a relentless pursuit of excellence. Along the way, mistakes will be made, bugs will be encountered, and frustrations will abound. But through it all, the dedicated coder perseveres, honing their craft with each line of code written and each problem solved.', 'In the grand tapestry of human creativity, the art of coding stands as a testament to the ingenuity and innovation of the human mind. From the simplest scripts to the most complex software systems, each line of code represents a brushstroke in the ever-evolving masterpiece of technology. So, let us embrace the artistry of coding, not merely as a means to an end but as a journey of discovery, creativity, and expression in the digital age.']"
Don't Just Let It iPaaS: How To Get More Out of Your Digital Transformation,https://dzone.com/articles/dont-just-let-it-ipaas-how-to-get-more-out-of-your-1,"[""Have you ever wondered what gives the cloud an edge over legacy technologies? When answering that question, the obvious but often overlooked aspect is the seamless integration of disparate systems, applications, and data sources. That's where Integration Platform as a Service (iPaaS) comes in.\xa0"", ""In today's complex IT landscape, your organization is faced with a myriad of applications, systems, and data sources, both on-premises and in the cloud. This means you face the challenge of connecting these disparate elements to enable seamless communication and data exchange. By providing a unified platform for integration, iPaaS enables you to break down data silos, automate workflows and unlock the full potential of your digital assets.\xa0"", 'Because of this, iPaaS is the unsung hero of modern enterprises. It can play a pivotal role in your digital transformation journey by streamlining and automating workflows. iPaaS also enables you to modernize legacy systems, enhance productivity, and create better experiences for your customers, users, and employees.\xa0', ""Let's explore some key tenets of how iPaaS accelerates digital transformation:"", ""iPaaS is not just a technology solution; it's a strategic enabler of digital transformation as it empowers organizations to adapt, innovate, and thrive in the digital age. In that way, it acts as a catalyst for digital transformation. By embracing iPaaS, you can break down barriers, enhance collaboration, and create a connected ecosystem that drives growth and customer satisfaction.\xa0""]"
Empowering ADHD Research With Generative AI: A Developer's Guide to Synthetic Data Generation,https://dzone.com/articles/empowering-adhd-research-with-generative-ai-a-deve,"[""Attention Deficit Hyperactivity Disorder (ADHD) presents a complex challenge in the field of neurodevelopmental disorders, characterized by a wide range of symptoms such as inattention, hyperactivity, and impulsivity that significantly affect individuals' daily lives. In the era of digital healthcare transformation, the role of artificial intelligence (AI), and more specifically Generative AI, has become increasingly pivotal. For developers and researchers in the tech and healthcare sectors, this presents a unique opportunity to leverage the power of AI to foster advancements in understanding, diagnosing, and treating ADHD."", ""From a developer's standpoint, the integration of Generative AI into ADHD research is not just about the end goal of improving patient outcomes but also about navigating the intricate process of designing, training, and implementing AI models that can accurately generate synthetic patient data. This data holds the key to unlocking new insights into ADHD without the ethical and privacy concerns associated with using real patient data. The challenge lies in how to effectively capture the complex, multidimensional nature of ADHD symptoms and treatment responses within these models, ensuring they can serve as a reliable foundation for further research and development."", 'Generative AI refers to a subset of AI algorithms capable of generating new data instances similar but not identical to the training data. This article proposes utilizing Generative Adversarial Networks (GANs) to generate synthetic patient data, aiding in the research and understanding of ADHD without compromising patient privacy.', 'Data will be synthetically generated to resemble real patient data, including symptoms, genetic information, and response to treatment. Preprocessing steps involve normalizing the data and ensuring it is suitable for training the GAN model.', 'The GAN consists of two main components: the Generator, which generates new data instances, and the Discriminator, which evaluates them against real data. The training process involves teaching the Generator to produce increasingly accurate representations of ADHD patient data.', 'Generated data can be used to identify patterns in ADHD symptoms and responses to treatment, contributing to more personalized and effective treatment strategies.', 'The application of Generative AI in ADHD research could lead to significant advancements in personalized medicine, early diagnosis, and the development of new treatment modalities. However, the accuracy of the generated data and the ethical implications of synthetic data use are important considerations.', 'This exploration opens up possibilities for using Generative AI to understand complex disorders like ADHD more deeply. Future research could focus on refining the models for greater accuracy and exploring other forms of AI to support healthcare professionals in diagnosis and treatment.', 'Generative AI has the potential to revolutionize the approach to ADHD by generating new insights and aiding in the development of more effective treatments. While there are challenges to overcome, the benefits to patient care and research could be substantial.']"
Neural Network Representations,https://dzone.com/articles/Neural-Network-Representations,"['Trained neural networks arrive at solutions that achieve superhuman performance on an increasing number of tasks. It would be at least interesting and probably important to understand these solutions.\xa0', 'Interesting, in the spirit of curiosity and getting answers to questions like, “Are there human-understandable algorithms that capture how object-detection nets work?”[a] This would add a new modality of use to our relationship with neural nets from just querying for answers (Oracle Models) or sending on tasks (Agent Models) to acquiring an enriched understanding of our world by studying the interpretable internals of these networks’ solutions (Microscope Models). [1]', 'And important in its use in the pursuit of the kinds of standards that we (should?) demand of increasingly powerful systems, such as operational transparency, and guarantees on behavioral bounds. A common example of an idealized capability we could hope for is “lie detection” by monitoring the model’s internal state. [2]', 'Mechanistic interpretability (mech interp) is a subfield of interpretability research that seeks a granular understanding of these networks.', 'One could describe two categories of mech interp inquiry:', '', 'Figure 1: “A Conscious Blackbox,"" the cover graphic for James C. Scott’s Seeing Like a State (1998)', 'This post is concerned with representation interpretability. Structured as an exposition of neural network representation research [b], it discusses various qualities of model representations which range in epistemic confidence from the obvious to the speculative and the merely desired.', 'Notes:', 'Now, to some foundational hypotheses about neural network representations.', 'The representations of inputs to a model are a composition of encodings of discrete information. That is, when a model looks for different qualities in an input, the representation of the input in some component of the model can be described as a combination of its representations of these qualities. This makes (de)composability a corollary of “encoding discrete information”- the model’s ability to represent a fixed set of different qualities as seen in its inputs.', '', 'Figure 2: A model layer trained on a task that needs it to care about background colors (trained on only blue and red) and center shapes (only circles and triangles)', 'The component has dedicated a different neuron to the input qualities: ""background color is composed of red,"" ""background color is composed of blue,"" ""center object is a circle,” and “center object is a triangle.”', ""Consider the alternative: if a model didn't identify any predictive discrete qualities of inputs in the course of training. To do well on a task, the network would have to work like a lookup table with its keys as the bare input pixels (since it can’t glean any discrete properties more interesting than “the ordered set of input pixels”) pointing to unique identifiers. We have a name for this in practice: memorizing. Therefore, saying, “Model components learn to identify useful discrete qualities of inputs and compose them to get internal representations used for downstream computation,” is not far off from saying “Sometimes, neural nets don’t completely memorize.”"", '', 'Figure 3: An example of how learning discrete input qualities affords generalization or robustness', 'This example test input, not seen in training, has a representation expressed in the learned qualities. While the model might not fully appreciate what “purple” is, it’ll be better off than if it was just trying to do a table lookup for input pixels.', 'Revisiting the hypothesis:', '""The representations of inputs to a model are a composition of encodings of discrete information.""', 'While, as we’ve seen, this verges on the obvious; it provides a template for introducing stricter specifications deserving of study.', 'The first of these specification revisits looks at “…are a composition of encodings…” What is observed, speculated, and hoped for about the nature of these compositions of the encodings?', 'To recap decomposition, we expect (non-memorizing) neural networks to identify and encode varied information from input qualities/properties. This implies that any activation state is a composition of these encodings.', '', 'Figure 4: What the decomposability hypothesis suggests', 'What is the nature of this composition? In this context, saying a representation is linear suggests the information of discrete input qualities are encoded as directions in activation space and they are composed into a representation by a vector sum:', '', 'We’ll investigate both claims.', 'Composability already suggests that the representation of input in some model components (a vector in activation space) is composed of discrete encodings of input qualities (other vectors in activation space). The additional thing said here is that in a given input-quality encoding, we can think of there being some core essence of the quality which is the vector’s direction. This makes any particular encoding vector just a scaled version of this direction (unit vector.)', '', 'Figure 5: Various encoding vectors for the red-ness quality in the input', 'They are all just scaled representations of some fundamental red-ness unit vector, which specifies direction.', 'This is simply a generalization of the composability argument that says neural networks can learn to make their encodings of input qualities ""intensity""-sensitive by scaling some characteristic unit vector.', '', 'Figure 6a', 'An alternative encoding scheme could be that all we can get from models are binary encodings of properties; e.g., “The Red values in this RGB input are Non-zero.” This is clearly not very robust.', '', 'Another is that we have multiple unique directions for qualities that could be described by mere differences in scale of some more fundamental quality: “One Neuron for ""kind-of-red"" for 0-127 in the RGB input, another for ""really-red"" for 128-255 in the RGB input.” We’d run out of directions fairly quickly.', 'Now, this is the stronger of the two claims as it is not necessarily a consequence of anything introduced thus far.', '', 'Figure 7: An example of 2-property representation', 'Note: We assume independence between properties, ignoring the degenerate case where a size of zero implies the color is not red (nothing).', 'A vector sum might seem like the natural (if not only) thing a network could do to combine these encoding vectors. To appreciate why this claim is worth verifying, it’ll be worth investigating if alternative non-linear functions could also get the job done. Recall that the thing we want is a function that combines these encodings at some component in the model in a way that preserves information for downstream computation. So this is effectively an information compression problem.', 'As discussed in Elhage et al [3a], the following non-linear compression scheme could get the job done:', '', 'Where we seek to compress values x and y into t. The value of Z is chosen according to the required floating-point precision needed for compressions.', '', 'As demonstrated, we’re able to compress and recover vectors a and b just fine, so this is also a viable way of compressing information for later computation using non-linearities like the floor() function that neural networks can approximate. While this seems a little more tedious than just adding vectors, it shows the network does have options. This calls for some evidence and further arguments in support of linearity.', 'The often-cited example of a model component exhibiting strong linearity is the embedding layer in language models [4], where relationships like the following exist between representations of words:', '', 'This example would hint at the following relationship between the quality of $plurality$ in the input words and the rest of their representation:\xa0', '', 'Okay, so that’s some evidence for one component in a type of neural network having linear representations. The broad outline of arguments for this being prevalent across networks is that linear representations are both the more natural and performant [3b][3a] option for neural networks to settle on.\xa0', 'If non-linear compression is prevalent across networks, there are two alternative regimes in which networks could operate:\xa0', '', 'Figure 9: Direct non-linear computation', 'As promised in the introduction, after avoiding the word “feature” this far into the post, we’ll introduce it properly. As a quick aside, I think the engagement of the research community on the topic of defining what we mean when we use the word “feature” is one of the things that makes mech interp, as a pre-paradigmatic science, exciting. While different definitions have been proposed [3c] and the final verdict is by no means out, in this post and others to come on mech interp, I’ll be using the following:\xa0', '""The features of a given neural network constitute a set of all the input qualities the network would dedicate a neuron to if it could.""', 'We’ve already discussed the idea of networks necessarily encoding discrete qualities of inputs, so the most interesting part of the definition is, “...would dedicate a neuron to if it could.”', 'In a case where all quality-encoding directions are unique one-hot vectors in activation space ([0, 1] and [1, 0], for example) the neurons are said to be basis-aligned; i.e., one neuron’s activation in the network independently represents the intensity of one input quality.', '', 'Note that while sufficient, this property is not necessary for lossless compression of encodings with vector addition. The core requirement is that these feature directions be orthogonal. The reason for this is the same as when we explored the non-linear compression method: we want to completely recover each encoded feature downstream.', 'Following the Linearity hypothesis, we expect the activation vector to be a sum of all the scaled feature directions:', '', 'Given an activation vector (which is what we can directly observe when our network fires), if we want to know the activation intensity of some feature in the input, all we need is the feature’s unit vector, feature^j_d: (where the character “.” \xa0in the following expression is the vector dot product.)', '', 'If all the feature unit vectors of that network component (making up the set, Features_d) are orthogonal to each other:', '', 'And, for any vector:', '', 'These simplify our equation to give an expression for our feature intensity feature^j_i:', '', 'Allowing us to fully recover our compressed feature:', '', 'All that was to establish the ideal property of orthogonality between feature directions. This means even though the idea of “one neuron firing by x-much == one feature is present by x-much” is pretty convenient to think about, there are other equally performant feature directions that don’t have their neuron-firing patterns aligning this cleanly with feature patterns. (As an aside, it turns out basis-aligned neurons don’t happen that often. [3d])', '', 'Fig 11: Orthogonal feature directions from non-basis-aligned neurons', 'With this context, the request: ”dedicate a neuron to…” might seem arbitrarily specific. Perhaps “dedicate an extra orthogonal direction vector” would be sufficient to accommodate an additional quality. But as you probably already know, orthogonal vectors in a space don’t grow on trees. A 2-dimensional space can only have 2 orthogonal vectors at a time, for example. So to make more room, we might need an extra dimension, i.e [X X] -> [X X X] which is tantamount to having an extra neuron dedicated to this feature.', 'To touch grass quickly, what does it mean when a model component has learned 3 orthogonal feature directions {[1 0 0], [0 1 0], [0 0 1]} for compressing an input vector [a b c]?', 'To get the compressed activation vector, we expect a series of dot products with each feature direction to get our feature scale.', '', 'Now we just have to sum up our scaled-up feature directions to get our “compressed” activation state. In this toy example, the features are just the vector values so lossless decompressing gets us what we started with.', '', 'The question is: what does this look like in a model? The above sequence of transformations of dot products followed by a sum is equivalent to the operations of the deep learning workhorse: matrix multiplication.', '', 'The earlier sentence, “…a model component has learned 3 orthogonal feature directions,” should have been a giveaway. Models store their learnings in weights, and so our feature vectors are just the rows of this layer’s learned weight matrix, W.', '', 'Why didn’t I just say the whole time, “Matrix multiplication. End of section.” Because we don’t always have toy problems in the real world. The learned features aren’t always stored in just one set of weights. It could (and usually does) involve an arbitrarily long sequence of linear and non-linear compositions to arrive at some feature direction (but the key insight of decompositional linearity is that this computation can be summarised by a direction used to compose some activation). The promise of linearity we discussed only has to do with how feature representations are composed. For example, some arbitrary vector is more likely to not be hanging around for discovery by just reading one row of a layer’s weight matrix, but the computation to encode that feature is spread across several weights and model components. So we had to address features as arbitrary strange directions in activation space because they often are. This point brings the proposed dichotomy between representation and algorithmic interpretability into question.', 'Back to our working definition of features:', '""The features of a given neural network constitute a set of all the input qualities the network would dedicate a neuron to if it could.""', 'You can think of this definition of a feature as a bit of a set-up for an introduction to a hypothesis that addresses its counterfactual: What happens when a neural network cannot provide all its input qualities with dedicated neurons?', 'Thus far, our model has done fine on the task that required it to compress and propagate 2 learned features — “size” and “red-ness” — through a 2-dimensional layer. What happens when a new task requires the compression and propagation of an additional feature like the x-displacement of the center of the square?', '', 'Figure 12', 'This shows our network with a new task, requiring it to propagate one more learned property of the input: center x-displacement. We’ve returned to using neuron-aligned bases for convenience.', 'Before we go further with this toy model, it would be worth thinking through if there are analogs of this in large real-world models. Let’s take the large language model GPT2 small [5]. Do you think, if you had all week, you could think of up to 769 useful features of an arbitrary 700-word query that would help predict the next token (e.g., “is a formal letter,"" “contains how many verbs,"" “is about about ‘Chinua Achebe,’” etc.)? Even if we ignored the fact that feature discovery was one of the known superpowers of neural networks [c] and assumed GPT2-small would also end up with only 769 useful input features to encode, we’d have a situation much like our toy problem above. This is because GPT2 has —at the narrowest point in its architecture— only 768 neurons to work with, just like our toy problem has 2 neurons but needs to encode information about 3 features. [d]', 'So this whole “model component encodes more features it has neurons” business should be worth looking into. It probably also needs a shorter name. That name is the Superposition hypothesis. Considering the above thought experiment with GPT2 Small, it would seem this hypothesis is just stating the obvious- that models are somehow able to represent more input qualities (features) than they have dimensions for.', ""There’s a reason I introduced it this late in the post: it depends on other abstractions that aren't necessarily self-evident. The most important is the prior formulation of features. It assumes linear decomposition- the expression of neural net representations as sums of scaled directions representing discrete qualities of their inputs. These definitions might seem circular, but they’re not if defined sequentially:"", 'If you conceive of neural networks as encoding discrete information of inputs called Features as directions in activation space, then when we suspect the model has more of these features than it has neurons, we call this Superposition.', ""As we’ve seen, it would be convenient if the features of a model were aligned with neurons and necessary for them to be orthogonal vectors to allow lossless recovery from compressed representations. So to suggest this isn't happening poses difficulties to interpretation and raises questions on how networks can pull this off anyway."", 'Further development of the hypothesis provides a model for thinking about why and how superposition happens, clearly exposes the phenomenon in toy problems, and develops promising methods for working around barriers to interpretability [6]. More on this in a future post.', '[a] That is, algorithms more descriptive than “Take this Neural Net architecture and fill in its weights with these values, then do a forward pass.”', '[b] Primarily from ideas introduced in Toy Models of Superposition', '[c] This refers specifically to the codification of features as their superpower. Humans are pretty good at predicting the next token in human text; we’re just not good at writing programs for extracting and representing this information vector space. All of that is hidden away in the mechanics of our cognition.', '[d] Technically, the number to compare the 768-dimension residual stream width to is the maximum number of features we think *any* single layer would have to deal with at a time. If we assume equal computational workload between layers and assume each batch of features was built based on computations on the previous, for the 12-layer GPT2 model, this would be 12 * 768 = 9,216 features you’d need to think up.', '[1] Chris Olah on Mech Interp - 80000 Hours\xa0', '[2] Interpretability Dreams\xa0', '[3] Toy Models of Superposition', '[3a] Nonlinear Compression\xa0', '[3b] Features as Directions\xa0', '[3c] What are Features?\xa0', '[3d] Definitions and Motivation', '[4] Linguistic regularities in continuous space word representations: Mikolov, T., Yih, W. and Zweig, G., 2013. Proceedings of the 2013 conference of the North American chapter of the Association for Computational Linguistics: Human language technologies, pp. 746--751.', '[5] Language Models are Unsupervised Multitask Learners: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever', '[6] Towards Monosemanticity: Decomposing Language Models With Dictionary Learning']"
The Impact of Artificial Intelligence on Customer Service,https://dzone.com/articles/the-impact-of-artificial-intelligence-on-customer,"['Artificial intelligence (AI) is revolutionizing customer service across industries by enabling efficient issue resolution, personalized recommendations, and omnichannel integration. AI-powered self-service, intelligent chatbots, sentiment analysis, predictive modeling, process automation, and data-driven insights are key technologies improving customer satisfaction and operational efficiency. Leading brands across retail, telecom, financial services, healthcare, and other sectors are using AI to transform their contact centers and customer support functions.\xa0', ""This article will analyze the breadth of AI adoption for customer service, benefits attained through reduced call volumes and enhanced issue containment, constructive use cases for total experience delivery, implementation challenges related to data integrity and ethical concerns, best practices for change management, workforce enablement, and responsible AI governance. Global examples from Microsoft, American Express, Disney, and Anthem demonstrate AI's transformative impact in building next generation customer service ecosystems geared for higher productivity and predictive support at scale. This article also looks ahead to examine AI's expanding role in customer intelligence, predictive analytics, service ecosystems, and creating sustainable competitive differentiation going forward."", 'Powerful natural language processing (NLP) enables search driven self-service portals to accurately interpret customer questions and provide relevant information proactively from knowledge bases, reducing dependency on live agents for repetitive queries. AI analyzes customer journeys across channels to recommend best next actions through intelligent navigation prediction.\xa0', 'AI-powered chatbots mimic human conversations using NLP and machine learning (ML), providing round-the-clock automated support for customers. Bots seamlessly integrate with backend systems to handle customer account services, product information, balances, transaction details, raising requests, and contextual FAQs delivering always-on support. Intelligent routing resolves simple queries immediately while referring intricate cases requiring empathy or human judgement to agents.\xa0', 'By applying ML on structured and unstructured data across customer interactions and account activities, AI models uncover usage patterns to accurately predict optimal products and proactively recommend best next actions without explicit customer input. Predictive purchase analytics provide hyper-personalized recommendations driving higher average order value and lifetime value. \xa0 \xa0', 'Voice-based interfaces leverage automatic speech recognition and NLP enable hands-free customer self-service over phone calls. This facilitates accessing information or conducting transactions through voice commands for use cases ranging from product catalog inquiries to technical troubleshooting. AI assistants route calls appropriately between self-service or live transfer based on conversational complexity. \xa0', 'AI helps map and analyze how customers interact with brands across purchase journeys spanning multiple channels like mobile apps, online chat, website navigation, phone calls, and physical stores. Touchpoint analytics then enable tighter journey orchestration through triggered messages and channel handovers optimized for individual preferences and conversion probability.', 'Real-time analysis of transactions, customer data, and behavior patterns by AI agents help identity threats and fraudulent activities accurately to prevent bad actors without deterring genuine customers through false declines. Adaptive deep learning algorithms get updated continuously by identifying emerging attack vectors and risk indicators. \xa0', 'Natural language and speech analysis accurately interprets customer issues for auto-categorization based on domain taxonomies. Solutions leverage dynamic knowledge graphs mapping millions of troubleshooting combinations for expert level diagnosis. Matching issues to available human specialists reduces delays, allowing instant transfers and closure through optimal routing.\xa0', 'Chatbots interface with process automation tools to initiate back-end workflows for actions like raising claims, scheduling field technician visits, processing refunds, onboarding customers onto new plans, or activating software licenses without agent assistance. This shifts contact center focus toward complex complaints and relationship building.', 'Agents are augmented with AI-generated knowledge, recommendations, and macros during customer interactions to improve productivity. Contextual information surfaces insights from past interactions and CRM records enabling personalization. Embedded training guides agents through best next actions for tricky cases based on historical resolutions and expert heuristics.\xa0', 'Human-AI co-piloting arrangements coupled with explainable models surface new insights from voice, text, and operational analytics — identifying areas needing enhancement across self-service funnels, policies, customer segmentation, and persona strategies. Quantifiable metrics help prioritize investments benefiting customer experience and business KPIs. \xa0', 'Despite immense potential, AI-led customer service transformation necessitates evolving contact centers, data architectures, and addressing ethical concerns — requiring upfront investment.', 'AI specialists need to be onboarded for building, validating, and maintaining AI endpoints like virtual assistants. Support functions undergo structural changes as tier-1 queries shift to self-service. Training programs skill up agents on collaborating with AI and focusing on relationship building. \xa0', 'Building reliable AI models requires significant volumes of high-quality structured data around customer interactions, operational metrics, product details, and transactional flows connected across enterprise systems — necessitating data infrastructure revamps for collection, organization, and aggregation into cloud data lakes.\xa0', 'Datasets may harbor societal biases skewing AI recommendations against protected groups. Continual fairness testing and controls must ensure inclusive unbiased treatment for customer experiences and backend workflows involving actions like credit limits or claims approval.\xa0', 'Explainable AI provides visibility into model confidence scores and rationale behind actions to assure customers. Permission and consent protocols enable voluntary choice in data collection securing privacy rights. Providing intuitive overrides for recommendations maintains autonomy and psychological comfort.', 'Proactive change management helps alleviate organizational resistance, smooth workflows, and reinforce customer centricity.', 'Underscore AI’s role in augmenting staff productivity and highlighting human judgement in complex cases rather than pure automation and headcount displacement. Embrace opportunities to reskill and redeploy displaced resources for specialized roles. \xa0\xa0', 'Realign agent evaluation metrics encouraging collaboration with AI tools and creative solutions. Discourage overdependence on prescribed recommendations. Maintain focus on empathy-driven service quality.\xa0', 'Establish feedback loops and communities of practice for agents to collaboratively advance AI augmentation techniques benefitting common use cases. Accelerate expertise development that maximizes mutual strengths. \xa0', 'Global enterprises exhibit measurable improvements across customer satisfaction, operational efficiency, and revenue growth indicators through AI adoption — quantifying technology capability into strategic differentiation.\xa0', 'Powered conversational AI abilities including language understanding, dialogue flow, mixed initiatives, and integrated connections to backend systems across customer touchpoints — driving 8x return on investment through $12 million in annual savings from automated resolutions and increased agent productivity worth over $100 million.\xa0', 'Virtual assistants handle 1.7 million customer inquiries monthly through natural conversations and account services accelerating issue resolution by quickly accessing records and initiating real-time notifications or transfers. 70% containment rate minimizes transfers and live chats.\xa0', 'Disney AI leverages semantic understanding of travel details and CRM integrations to unlock seamless brand interactions across reservations for theme park visits, cruise bookings, and hotel reservations — forecast to enable over $40 million value through enhanced booking rates and lowered service costs.', 'Voice-based digital assistants on mobile apps and interactive voice response systems efficiently guide over 40% of user inquiries on health plans, benefits, and claim statuses round the clock — reducing calls into call centers allowing cost savings and improved customer satisfaction from quicker hassle free resolutions.', 'While current AI use cases target challenges around efficiency, consistency, and personalization, exponential technological advances expand future possibilities dramatically in regards to hyper-realistic conversations, emotional intelligence, and generative content, blurring lines between human and artificial capabilities.', 'AI assistants achieve human parity in unstructured conversations through advances in generative linguistics modeling — mastering context, multiple intents, interruptions, and complex multi-step dialogues spanning domains. Photorealistic avatars manifest trust through nonverbal cues. \xa0', 'Voice analysis and facial emotions analytics achieve empathetic support and stress mitigation through appropriate responses. Tone analysis guides agents on de-escalation tactics improving satisfaction. Emotion simulation makes interactions more natural and intriguing. \xa0', 'Automated self-service platforms generate bespoke content like bank statements, insurance policies, invoices, and analytical summaries personalized to customers — saving enormous manual effort. AR/VR enables interactive 3D visualization and simulations. \xa0', ""AI's current exponential trajectory may see progress toward artificial general intelligence (AGI) — achieving generalized learning, situational awareness, and reasoning on par with human cognition. AGI manifesting as a helpful collaborative entity alongside humans may assist organizations in achieving customer support capabilities that were previously unfathomable. But balancing business needs against ethics in such emerging paradigms remains crucial."", 'From predictive recommendations to emotionally aware conversations, artificial intelligence unlocks immense potential in taking customer experience to unprecedented levels.']"
Scaling Redis Without Clustering,https://dzone.com/articles/scaling-redis-without-clustering,"[""Redis is a popular in-memory data store known for its speed and flexibility. It operates efficiently when a particular workload's memory and computational demands can be managed within a single node. However, scaling beyond a single node often leads to the consideration of the Redis Cluster. There's a common assumption that transitioning to the Redis Cluster is straightforward and existing applications will behave the same, but that's not the case in reality. While Redis Cluster addresses certain scaling issues, it also brings significant complexities. This post will discuss the limitations of scaling with Redis Cluster, and introduce some simpler alternatives that may meet many organizations' needs."", 'Redis Cluster is a distributed implementation that allows you to share your data across multiple primary Redis instances automatically and thus scale horizontally. In a Redis Cluster setup, the keyspace is split into 16384 hash slots (read more here), effectively setting an upper limit for the cluster size of 16384 master instances. However, in practice, the suggested maximum size is on the order of ~1000 master instances. Each master instance within the cluster manages a specific subset of these 16,384 hash slots. To ensure high availability, each primary instance can be paired with one or more replica instances. This approach, involving data sharding across multiple instances for scalability and replication for redundancy, is a common pattern in many distributed data storage systems. A cluster having three primary Redis instances, with each primary instance having one replica, is shown below:', '', 'As you can see, Redis Cluster has an elegant design. But without diving too deeply into the implementation details, we have already introduced multiple concepts (sharding, hash slots, etc.), just to have a basic understanding. We will explore more challenges below.', 'While Redis Cluster is often thought of as merely a setting on standalone Redis it differs in many ways. This means that migrating to the Redis Cluster is filled with hidden challenges that can surprise even experienced developers.', ""From the perspective of client libraries, working with Redis Cluster involves adapting to a different interface. That means you will very likely need to use a cluster-specific client library - your existing client for standalone Redis will not work out-of-box with a cluster. Let's take the go-redis Redis is an example. When connecting to a standalone Redis, the following code snippet can be used to connect to an instance with a specified host, password, and logical database:"", '', ""The same package offers a cluster-specific client, but it's important to note that they are distinct structures: redis.Client for standalone Redis and redis.ClusterClient for Redis Cluster. The initialization process for redis.ClusterClient is different as shown below. This distinction is crucial because redis.ClusterClient internally utilizes multiple redis.Client objects to manage communications with all the nodes in a cluster. Each redis.Client in this setup maintains its separate pool of connections."", '', ""A side note that the go-redis package also provides a redis.UniversalClient interface to simplify the initialization process, but it's just a layer of abstraction on top of different types of clients. To work with Redis Cluster, a cluster-specific client as demonstrated above needs to be developed to take care of additional requirements from a cluster."", 'Multi-key operations in Redis refer to commands or sets of commands that act on more than one key simultaneously. For instance, the MSET command is a single command that can potentially set multiple keys to multiple values in a single atomic operation. Similarly, transactions or Lua scripts often involve multiple keys, though not always. A key limitation of the Redis Cluster is that it cannot handle multi-key operations unless the involved keys share the same hashtag, as outlined in the specification here. You need to carefully review all the multi-key operations in your application, and these operations need to be modified to avoid using multi-key operations or only use them in the context of the same hashtag.', ""In Redis, a logical database is essentially a separate keyspace within the same Redis instance. Redis supports multiple databases, which are identified by a database index, a simple integer. Unless otherwise specified, Redis clients normally use the default database, database 0, upon connecting. Utilizing logical databases can be an effective strategy for separating different types of data (e.g., isolating session data from application data). You can use the SELECT command to switch between different logical databases. However, as specified in the SELECT command documentation, you lose this isolation with Redis Cluster. Therefore, if you're transitioning to Redis Cluster, it necessitates a complete re-architecture of your keyspace schema from scratch."", 'What we have discussed above was just the application level. However, the implications of clustering extend into infrastructure complexity and operational costs.', 'First of all, enabling the Redis Cluster substantially increases infrastructure complexity and the operational burden for teams managing it. Instead of monitoring metrics and availability on a single Redis instance, you need to do it across dozens or hundreds of nodes.', 'Secondly, workloads should be distributed evenly across the cluster while minimizing hotspots in particular instances. In reality, however, this does not happen very frequently. A common misconception is that creating a cluster will automatically lead to uniform traffic across all shards. Yet, specific slots can almost always get more traffic than others. While Redis allows you to reshard the instances, it has no automatic rebalancing features.', ""Last but not least, with more instances come more potential points of failure. While a cluster provides failover capabilities when nodes go down, the multiplied risk is real. Let's assume any single Redis instance has a 0.01% chance of failure. At 100 instances, that escalates to a 1-2% chance, meaning more incidents, fire drills, and potentially sleepless nights."", ""Given the challenges of Redis Cluster, it's worth considering alternatives that can provide a simpler path to scaling Redis."", ""Tune application access patterns to Redis can significantly minimize the working data size. With small tweaks to keys, data structures, and queries, you may squeeze a lot more out of your existing Redis deployment. For example, at Abnormal Security, they shortened their 64-character key strings to 32 characters. This reduced the per-key memory footprint, allowing them to fit 15% more key-value pairs. However, application-level optimization is not always feasible. After delving deep into the optimization process, it can sometimes feel like you're serving the needs of the data store rather than leveraging it to support your business goals."", ""If throughput is not an issue, and you just need more memory, the simplest solution is upgrading to a higher memory instance. Of course, this means that the extra CPUs that come with this beefier instance will sit idle and will not join the party. Sometimes it's annoying to pay for them but at least you are not contributing to global warming."", 'However, since Redis is single-threaded, if your workload is CPU-bound – for instance, involving extensive parsing and modification of JSON data – simply adding more CPUs will probably not help.', 'Redis on Flash, also known as data tiering, is not available in the Redis open-source version and is a relatively new feature with Redis-managed solutions. Data tiering allows parts of a Redis dataset that are not frequently accessed to be moved to an SSD storage tier attached to the instance. This provides significantly more memory capacity (up to 2TB of combined in-memory and SSD storage on certain cloud platforms) for the working dataset while keeping the hot dataset in low-latency main memory.', 'Since Redis on Flash keeps all keys in memory and only stores the values to disk as needed, Redis on Flash is only a good choice if your key size is substantially smaller than your value size on average. As an example, if 100-byte keys are storing 120-byte values, there is minimal memory savings to be gained from using flash storage.', ""Moreover, the common principle of accessing 20% of your dataset 80% of the time applies to the total data volume, including both keys and values. Thus, Redis on Flash doesn't help if you are retrieving only 20% of the keys, but those keys correspond to 80% of the values."", '', 'Besides the techniques mentioned above, another alternative is adopting a more modern Redis implementation. Dragonfly is a simple, performant, and cost-efficient in-memory data store, offering full compatibility with Redis APIs. Dragonfly allows you to continue scaling vertically at up to 128 cores and 1TB of memory. A single Dragonfly node can deliver a similar scale to that of a Redis Cluster but without the added operational complexity and limitations on Redis features discussed above.', 'Dragonfly is a drop-in Redis replacement that can meet scaling needs without requiring clustering. By utilizing a multi-threaded shared-nothing architecture on a single node, Dragonfly can handle large memory workloads and traffic spikes that would otherwise require clustering.', ""Some of Dragonfly's key benefits over a cluster setup include:"", ""When migrating an application from Redis to Dragonfly, it's important to recognize that some tuning may be required to fully realize Dragonfly's advantages. Applications built for single-threaded Redis are often optimized for minimal parallelism. Dragonfly, by contrast, is multi-threaded. It can be worthwhile to spend time tweaking your client code or configuration to increase the number of connections to Dragonfly and spread data out across more keys."", 'While the need for horizontal scaling is inevitable past a certain scale - usually 700GB or 1TB of memory - many organizations scale horizontally prematurely before exhausting vertical scaling options. This results in needless operational complexity for workloads that could have fit on a single, sufficiently powered node.', 'For many use cases, capacity limits are theoretical rather than practical: even if your business grows, sometimes the specific use case for which you keep data in Redis is more limited.', ""And even if you have to eventually scale horizontally, you'll still want to use relatively powerful instances with 16 or 32 cores rather than spreading the work over hundreds of tiny instances. In that case, each instance becomes a potential weak link: if traffic changes, or there is a sudden surge in throughput or load, the tiny instances won't be able to cope."", 'While Redis Cluster provides horizontal scaling, it comes at a high cost in terms of complexity and loss of functionality. Dragonfly can support millions of operations per second and terabyte-sized workloads on a single instance, circumventing many of the pitfalls of clustering solutions.']"
10 Bold Predictions for AI in 2024,https://dzone.com/articles/10-bold-predictions-for-ai,"[""With 2023 in the rearview mirror, it's fair to say that OpenAI's release of ChatGPT just over a year ago threw the tech industry into an excited, manic state. Companies like Microsoft and Google have thrown tremendous resources at AI in order to try to catch up, and VCs have tripped all over themselves to fund companies doing the same. With such a tremendous pace of innovation, it can be difficult to spot what's coming next, but we can try to take clues from AI's evolution so far to predict where it's headed. Here, we present 10 bold predictions laying out how emerging trends in AI development are likely to play out in 2024 and beyond."", ""While some consumers were awed by the introduction of ChatGPT, perhaps many more picked it up, played with it, and moved on with their lives. But in 2024, the former audience is likely to re-engage with the technology, as the trend towards personal AI will revolutionize user interactions with technology. These AI systems, trained on individual user data, offer highly personalized experiences and insights. For example, Google Gemini now integrates with users' Google Workspace data, enabling it to leverage everything it knows about their calendars, documents, location, chats and more. Meanwhile, companies like Apple and Samsung are likely to emphasize on-device AI as a key feature, prioritizing privacy and immediacy. It's not hard to imagine a personal AI with access to all of your data acting as a relationship, education, and career coach, becoming a more integral, personalized part of everyday life."", 'Long promised, the dream of ""data democratization"" will finally come to pass. AI is finally democratizing data access for business users by enabling them to ask data questions in plain English, eliminating the need to write queries in SQL. Cloud data platforms like Snowflake and Databricks are already falling all over themselves to integrate these features into the next generation of their product offerings.', 'As enterprises focus on centralizing data from diverse operational data stores in order to build a corpus to train AI on, data integration tools are becoming increasingly crucial. The emphasis is on tools that are user-friendly and capable of moving large volumes of data, while keeping that data in sync and up-to-date through the use of change data capture capabilities. As companies race towards AI, the need to leverage these tools to bypass large amounts of technical complexity and beat competitors to market will continue to grow in importance.', 'Data governance remains a red-hot topic in the enterprise data management space. Companies at the vanguard of this trend are establishing robust governance frameworks to ensure data quality, compliance and security. This trend reflects the growing awareness and importance of responsible data management practices in a data-rich world. Data catalogs, metadata tagging tools, and data quality tools will leverage AI to great effect, allowing companies to make sense of their data in a more organized, automated fashion.', 'Enterprises are increasingly conscious of AI safety and security, particularly in safeguarding brand reputation. Larger companies, having learned from early missteps like the Tay chatbot and biased recruiting tools, are increasingly keen to implement robust guardrails around publicly deployed AI. Their main focus will be balancing innovation with responsibility, aiming to avoid public missteps and ensure fairness and unbiased outcomes. Expect companies to be overly cautious at first, and fine-tune the balance as they become more comfortable.', 'Enterprises are increasingly favoring domain-specific, specialized AI models over general-purpose ones. While an LLM like ChatGPT is quite good at a broad range of general tasks, from writing poetry to summarizing emails, this is less interesting and useful for enterprises than specialized GPTs trained on focused datasets that are excellent in a single domain. For example, a healthcare company might train a GPT on its vast amounts of historical billing data, with the sole purpose of predicting costs with great precision. These tailored models offer greater accuracy and efficiency in enterprise applications, signifying a move towards more customized AI solutions in the corporate world.', 'The landscape of AI is witnessing a significant shift as open source models from Mistral, Anthropic, MosaicML and others rapidly improve, narrowing the gap with commercial counterparts like OpenAI. This trend is reshaping the AI ecosystem, making advanced AI tools more accessible and fostering a more competitive and diverse AI market. The pace of innovation seems to only be accelerating, as open source and commercial AIs alike look to come out on top in the AI gold rush.', ""Governments and businesses are increasingly focused on regulating AI due to growing concerns about its risks. Governments want to get ahead of the possibility of rogue AIs falling into the hands of bad actors, which could and threaten national security. Meanwhile, some industry players have signaled a desire to expand their moat by supporting rules that limit competition, while others are genuinely concerned about AI's potential to harm humanity. All of these groups see reasons to act, and as a result, new regulations are starting to take shape. The European Union has already set a precedent with landmark AI regulation that could serve as a model for the USA and others. What that final legislation looks like, though, remains to be seen."", 'Data lakes are growing at a rapid pace, and finally being taken seriously by large enterprises who are recognizing that they are needed to house large volumes of unstructured and semi-structured text data needed for AI. Data warehouses will still command the majority of market share, but data lakes will continue to grow at a much faster rate. The flexibility and scalability of data lakes make them increasingly attractive for managing large, varied datasets in modern data ecosystems, like those that companies are looking to bring together to train LLMs.', 'The process of fine-tuning AI models is becoming significantly easier, thanks to new AI platforms that promise a more user-friendly and refined experience. These new platforms will abstract away much of the complexity of fine-tuning, making model customization more accessible, and allowing a broader range of users to tailor AI models to specific needs and applications.', ""In the last few waves of highly hyped tech trends (I'm looking at you, crypto), the most prominent players in the space produced a similar amount of sound and fury. Critics argue that AI is no different, and that today's buzz will settle down once we realize that we need more advanced models to achieve artificial general intelligence (AGI). However, I believe AI is different. Unlike crypto, AI's practical benefits are already clear to most users, and yet we're only beginning to grasp how it will be used in innovative and transformative ways. The Age of AI is here, and my hope is that with these predictions, you'll begin to see where we're headed, and how profoundly I believe AI will alter the course of industry and humanity alike.""]"
Advanced Architecture for AI Application (AKA AAAA!),https://dzone.com/articles/advanced-architecture-for-ai-application-aka-aaaa,"['Surprise! This is a bonus blog post for the AI for Web Devs series I recently wrapped up. If you haven’t read that series yet, I’d encourage you to check it out.', 'This post will look at the existing project architecture and ways we can improve it for both application developers and the end user.', 'I’ll be discussing some general concepts, and using specific Akamai products in my examples.', 'The existing application is pretty basic. A user submits two opponents, then the application streams back an AI-generated response of who would win in a fight.', 'The architecture is also simple:', 'I used Akamai’s cloud computing services (formerly Linode) but this would be the same for any hosting service.', '', 'Technically this works fine, but there are a couple of problems, particularly when users make duplicate requests. It could be faster and more cost-effective to store responses on our server and only go to OpenAI for unique requests.', 'This assumes we don’t need every single request to be non-deterministic (the same input produces a different output). Let’s assume it’s OK for the same input to produce the same output. After all, a prediction for who would win in a fight wouldn’t likely change.', 'If we want to store responses from OpenAI, a practical place to put them is in some sort of database that allows for quick and easy lookup using the two opponents. This way, when a request is made, we can check the database first:', '', 'With this setup, any duplicate requests will be handled by the database. By making some of the OpenAI requests optional, we can potentially reduce the amount of latency users experience, plus save money by reducing the number of API requests.', 'This is a good start, especially if the server and the database exist in the same region. It would make for much quicker response times than going to OpenAI’s servers.', 'However, as our application becomes more popular, we may start getting users from all over the world. Faster database lookups are great, but what happens if the bottleneck is the latency from the time spent in flight?', 'We can address that concern by moving things closer to the user.', 'If you’re not already familiar with the term “edge”, this part might be confusing, but I’ll try to explain it simply. Edge refers to content being as close to the user as possible. For some people, that could mean IoT devices or cellphone towers, but in the case of the web, the canonical example is a Content Delivery Network (CDN).', 'I’ll spare you the details, but a CDN is a network of globally distributed computers that can respond to user requests from the nearest node in the network (something I’ve written about in the past). While traditionally they were designed for static assets, in recent years, they started supporting edge computing (also something I’ve written about in the past).', 'With edge computing, we can move a lot of our backend logic super close to the user, and it doesn’t stop at computing. Most edge compute providers also offer some sort of eventually consistent key-value store in the same edge nodes.', 'How could that impact our application?', '', 'The origin server may not be strictly necessary here, but I think it’s more likely to be there. For the sake of data, compute, and logic flow, this is mostly the same as the previous architecture. The main difference is that the previously stored results now exist super close to users and can be returned almost immediately.', '(Note: although the data is being cached at the edge, the response is still dynamically constructed. If you don’t need dynamic responses, it may be simpler to use a CDN in front of the origin server and set the correct HTTP headers to cache the response. There are a lot of nuances here, and I could say more but…well, I’m tired and don’t want to. Feel free to reach out\xa0if you have any questions.)', 'Now we’re cooking! Any duplicate requests will be responded to almost immediately, while also saving us unnecessary API requests.', 'This sorts out the architecture for the text responses, but we also have AI-generated images.', 'The last thing we’ll consider today is images. When dealing with images, we need to think about delivery and storage. I’m sure that the folks at OpenAI have their solutions, but some organizations want to own the entire infrastructure for security, compliance, or reliability reasons. Some may even run their image generation services instead of using OpenAI.', 'In the current workflow, the user makes a request that ultimately makes its way to OpenAI. OpenAI generates the image but doesn’t return it. Instead, they return a JSON response with the URL for the image, hosted on OpenAI’s infrastructure. With this response, an <img> tag can be added to the page using the URL, which kicks off another request for the actual image.', 'If we want to host the image on our infrastructure, we need a place to store it. We could write the images onto the origin server’s disk, but that could quickly use up the disk space, and we’d have to upgrade our servers, which can be costly. Object storage is a much cheaper solution (I’ve also written about this). Instead of using the OpenAI URL for the image, we could upload it to our object storage instance and use that URL instead.', 'That solves the storage question, but object storage buckets are generally deployed to a single region. This echoes the problem we had with storing text in a database. A single region may be far away from users, which could cause a lot of latency.', 'Having introduced the edge already, it would be pretty trivial to add CDN features for just the static assets (frankly, every site should have a CDN). Once configured, the CDN will pull images from object storage on the initial request and cache them for any future requests from visitors in the same region.', 'Here’s how our flow for images would look:', '', 'This last architecture is, admittedly, a little bit more complex, but if your application is going to handle serious traffic, it’s worth considering.', 'Right on! With all those changes in place, we have created AI-generated text and images for unique requests and serve cached content from the edge for duplicate requests. The result is faster response times and a much better user experience (in addition to fewer API calls).', 'I kept these architecture diagrams applicable across various databases, edge compute, object storage, and CDN providers on purpose. I like my content to be broadly applicable. But it’s worth mentioning that integrating the edge is about more than just performance. There are a lot of really cool security features you can enable as well.', 'For example, on Akamai’s network, you can have access to things like web application firewalls (WAF), distributed denial of service (DDoS) protection, intelligent bot detection, and more. That’s all beyond the scope of today’s post, though.', 'So for now, I’ll leave you with a big “thank you” for reading. I hope you learned something. As always, feel free to reach out at any time with comments, questions, or concerns.', 'Thank you so much for reading. If you liked this article, and want to support me, the best ways to do so are to share it and follow me on Twitter.']"
The Power of Generative AI: How It Is Revolutionizing Business Process Automation,https://dzone.com/articles/the-power-of-generative-ai-business-process-automation,"['Generative AI, a type of AI that can create new data or content, is revolutionizing business process automation. By utilizing generative AI, businesses can streamline and enhance various processes, resulting in increased productivity, efficiency, and innovation. One of the key advantages of generative AI in business automation is its ability to speed up content creation.\xa0', 'With generative AI, businesses can produce high-quality writing in seconds, reducing the time and effort required to develop marketing copy, technical materials, or any other written materials. Generative AI can also assist in software development, generating code that is largely correct and instantaneous. This allows IT and software organizations to accelerate their development cycle, saving time and resources.\xa0', ""Moreover, generative AI can be used to improve data analysis and decision-making. Through generative models, businesses can generate new data that can be used for data augmentation and analysis, providing valuable insights and informing strategic decisions. Generative AI also has the potential to automate repetitive tasks and reduce human involvement, freeing up employees' time to focus on more complex and strategic activities."", 'Businesses are increasingly realizing the potential of generative AI in revolutionizing business efficiency. Generative AI can be leveraged to automate a wide range of business processes, from content creation to data analysis and decision-making. By harnessing the power of generative AI, organizations can achieve significant improvements in productivity, cost savings, and innovation.', 'One of the key areas where generative AI is transforming business efficiency is content creation. With generative AI, businesses can quickly produce high-quality written materials such as marketing copy, technical documents, and more. This not only saves time and resources but also enables businesses to maintain a consistent level of quality across their content.', ""Moreover, generative AI's impact extends to software development, where it can rapidly generate code, helping IT and software organizations accelerate their development cycles and minimize errors. Additionally, generative AI can significantly enhance data analysis and decision-making by providing valuable insights through the generation of new data for data augmentation and analysis."", 'Furthermore, the automation capabilities of generative AI can streamline repetitive tasks and reduce the need for human involvement, allowing employees to focus on more complex and strategic activities. Overall, the integration of generative AI in business processes holds great promise for optimizing efficiency and driving innovation. In short, generative AI has the potential to revolutionize business process automation by enabling organizations to produce clear written materials, generate code more efficiently, and enhance.', ""Generative AI has the potential to transform automation across various industries. In the field of marketing and advertising, generative AI can revolutionize content creation by generating high-quality marketing copy, ad variations, and even personalized content for targeted audiences. This can significantly reduce the time and effort required for content creation while enhancing marketing campaigns' effectiveness."", 'In the healthcare industry, generative AI can revolutionize medical image analysis by generating higher-resolution versions of medical images. This can greatly aid in diagnosing and treating patients, as healthcare professionals can have access to more detailed and accurate visual information. Furthermore, generative AI can play a crucial role in autonomous analytics, where it can learn and adapt to its environment to make optimal decisions with minimal human intervention.', 'In the finance and banking sector, generative AI can streamline and automate processes such as risk assessment and customer onboarding. Generative AI can analyze vast amounts of data to identify patterns and make predictions, enabling more accurate risk assessments and helping financial institutions make informed decisions. Additionally, generative AI has the potential to revolutionize Know Your Customer processes in the finance and banking sectors. By leveraging generative AI for identity verification and risk assessment, financial institutions can enhance the efficiency and accuracy of customer onboarding procedures while ensuring compliance with regulatory requirements. By generating personalized customer experiences, generative AI can also enhance customer engagement and satisfaction in the finance and banking industry.\xa0', 'In the gaming sector, generative AI can revolutionize game development and design. Generative AI can assist in creating realistic and immersive game environments, characters, and narratives. This can enhance the overall gaming experience for players and introduce new levels of creativity and unpredictability in game design. By leveraging generative AI, game developers can rapidly generate and iterate on content, reducing development time and costs.\xa0', 'In the supply chain industry, generative AI can optimize inventory management and demand forecasting. Generative AI can analyze historical data and generate accurate predictions for future demand, enabling businesses to optimize their inventory levels and minimize stockouts or overstocks. Furthermore, generative AI can also help optimize logistics and transportation routes, identifying the most efficient paths to reduce delivery time and costs.', 'In the insurance sector, generative AI can improve claims processing and fraud detection. Generative AI can analyze large volumes of data and quickly identify patterns and anomalies, allowing insurance companies to streamline their claims processing workflows and detect potential instances of fraud. Moreover, generative AI can also significantly impact invoice processing in the insurance sector. By analyzing and extracting relevant information from invoices, generative AI can streamline the processing of insurance claims and invoices, leading to faster turnaround times and improved accuracy in financial transactions.', ""While the potential of generative AI for automation across industries is significant, there are several challenges that organizations may face when adopting this technology. It's essential to address these challenges to successfully integrate and leverage generative AI for automation purposes."", 'Addressing these challenges will be crucial for organizations seeking to harness the full potential of generative AI for automation. By recognizing and mitigating these obstacles, businesses can successfully integrate this technology to drive efficiency and innovation across various industries.', 'In conclusion, the potential of generative AI to revolutionize business efficiency and transform automation across industries is undeniable. From content creation to data analysis, decision-making, and process automation, generative AI offers a wide range of applications that can bring about significant improvements in productivity, cost savings, and innovation for businesses.', 'As businesses continue to adopt generative AI technologies, they can look forward to streamlining operations, enhancing customer experiences, and driving continuous advancements in various sectors such as marketing and advertising, healthcare, finance and banking, gaming, supply chain, and insurance. The seamless integration of generative AI into business processes has the power to reshape industries and pave the way for a future where automation is more efficient, intelligent, and impactful.', 'While some challenges in adopting generative AI for automation may arise, the potential benefits far outweigh these obstacles. With the right strategies and considerations in place, businesses can overcome these challenges and unlock the full potential of generative AI to propel their organizations into a more innovative and efficient future. As generative AI continues to evolve and mature, businesses that embrace and harness its capabilities will be at the forefront of driving transformative changes in their respective industries.']"
AI for Web Devs: Your First API Request to OpenAI,https://dzone.com/articles/ai-for-web-devs-your-first-api-request-to-openai,"['Welcome back to the series where we are learning how to integrate AI products into web applications:', 'Last time, we got all the boilerplate work out of the way.', 'In this post, we’ll learn how to integrate OpenAI’s API responses into our Qwik app using fetch. We’ll want to make sure we’re not leaking API keys by executing these HTTP requests from a backend.', 'By the end of this post, we will have a rudimentary, but working AI application.', '', 'Generate OpenAI API Key', 'Before we start building anything, you’ll need to go to platform.openai.com/account/api-keys and generate an API key to use in your application.', 'Make sure to keep a copy of it somewhere because you will only be able to see it once.', 'With your API key, you’ll be able to make authenticated HTTP requests to OpenAI. So it’s a good idea to get familiar with the API itself. I’d encourage you to take a brief look through the OpenAI Documentation and become familiar with some concepts. The models are particularly good to understand because they have varying capabilities.', 'If you would like to familiarize yourself with the API endpoints, expected payloads, and return values, check out the OpenAI API Reference. It also contains helpful examples.', 'You may notice the JavaScript package available on NPM called openai. We will not be using this, as it doesn’t quite support some things we’ll want to do, that fetch can.', 'The application we’re going to build will make an AI-generated text completion based on the user input. For that, we’ll want to work with the chat endpoint (note that the completions endpoint is deprecated).', ""We need to make a POST request to https://api.openai.com/v1/chat/completions with the 'Content-Type' header set to 'application/json', the 'Authorization' set to 'Bearer OPENAI_API_KEY' (you’ll need to replace OPENAI_API_KEY with your API key), and the body set to a JSON string containing the GPT model to use (we’ll use gpt-3.5-turbo) and an array of messages:"", '', 'You can run this right from your browser console and see the request in the Network tab of your dev tools.', 'The response should be a JSON object with a bunch of properties, but the one we’re most interested in is the ""choices"". It will be an array of text completions objects. The first one should be an object with an ""message"" object that has a ""content"" property with the chat completion.', '', 'Congrats! Now you can request a mediocre joke whenever you want.', 'The fetch the request above is fine, but it’s not quite an application. What we want is something a user can interact with to generate an HTTP request like the one above.', 'For that, we’ll probably want some sort to start with an HTML <form> containing a <textarea>. Below is the minimum markup we need:', '', 'We can copy and paste this form right inside our Qwik component’s JSX template. If you’ve worked with JSX in the past, you may be used to replacing the for attribute \xa0<label> with htmlFor, but Qwik’s compiler doesn’t require us to do that, so it’s fine as is.', 'Next, we’ll want to replace the default form submission behavior. By default, when an HTML form is submitted, the browser will create an HTTP request by loading the URL provided in the form’s action attribute. If none is provided, it will use the current URL. We want to avoid this page load and use JavaScript instead.', 'If you’ve done this before, you may be familiar with the preventDefault method on the Event interface. As the name suggests, it prevents the default behavior for the event.', 'There’s a challenge here due to how Qwik deals with event handlers. Unlike other frameworks, Qwik does not download all the JavaScript logic for the application upon the first-page load. Instead, it has a very thin client that intercepts user interactions and downloads the JavaScript event handlers on-demand.', 'This asynchronous nature makes Qwik applications much faster to load but introduces the challenge of dealing with event handlers asynchronously. It makes it impossible to prevent the default behavior the same way as synchronous event handlers that are downloaded and parsed before the user interactions.', 'Fortunately, Qwik provides a way to prevent the default behavior by adding preventdefault:{eventName} to the HTML tag. A very basic form example may look something like this:', '', 'Did you notice that little $ at the end of the onSubmit$ handler, there? Keep an eye out for those, because they are usually a hint to the developer that Qwik’s compiler is going to do something funny and transform the code. In this case, it’s due to the lazy-loading event handling system I mentioned above.', 'Now we have the tools in place to replace the default form submission with the fetch request we created above.', 'What we want to do next is pull the data from the <textarea> into the body of the fetch request. We can do so with, which expects a form element as an argument and provides an API to access a form control values through the control’s name attribute.', 'We can access the form element from the event’s target property, use it to create a new FormData object, and use that to get the <textarea> value by referencing its name, “prompt”. Plug that into the body of the fetch request we wrote above, and you might get something that looks like this:', '', 'In theory, you should now have a form on your page that, when submitted, sends the value from the text to the OpenAI API.', 'Although our HTTP request is working, there’s a glaring issue. Because it’s being constructed on the client side, anyone can open the browser dev tools and inspect the properties of the request. This includes the Authorization header containing our API keys.', 'I’ve blocked out my API token here with a red bar. This would allow someone to steal our API tokens and make requests on our behalf, which could lead to abuse or higher charges on our account.', 'Not good!!!', 'The best way to prevent this is to move this API call to a backend server that we control that would work as a proxy. The frontend can make an unauthenticated request to the backend, and the backend would make the authenticated request to OpenAI and return the response to the frontend. However because users can’t inspect backend processes, they would not be able to see the Authentication header.', 'So how do we move the fetch request to the backend?', 'I’m so glad you asked!', 'We’ve been mostly focusing on building the front end with Qwik, the framework, but we also have access to Qwik City, the full-stack meta-framework with tooling for file-based routing, route middleware, HTTP endpoints, and more.', 'Of the various options Qwik City offers for running backend logic, my favorite is routeAction$. It allows us to create a backend function triggered by the client over HTTP (essentially an RPC endpoint).', 'The logic would follow:', 'A simplified example could be:', '', 'I included a JSON.stringify(action) at the end of the template because I think you should see what the returned ActionStore looks like. It contains extra information like whether the action is running, what the submission values were, what the response status is, what the returned value is, and more.', 'This is all very useful data that we get out of the box just by using an action, and it allows us to create more robust applications with less work.', ""Qwik City's actions are cool, but they get even better when combined with Qwik’s <Form> component:"", 'Under the hood, the component uses a native HTML element, so it will work without JavaScript.', 'When JS is enabled, the component will intercept the form submission and trigger the action in SPA mode, allowing to have a full SPA experience.', 'By replacing the HTML <form> element with Qwik’s <Form> component, we no longer have to set up preventdefault:submit, onSubmit$, or call action.submit(). We can just pass the action to the action prop and it’ll take care of the work for us. Additionally, it will work if JavaScript is not available for some reason (we could have done this with the HTML version as well, but it would have been more work).', '', 'So that’s an improvement for the developer experience. Let’s also improve the user experience.', 'Within the ActionStore, we have access to the isRunning data which keeps track of whether the request is pending or not. It’s handy information we can use to let the user know when the request is in flight.', 'We can do so by modifying the text of the submit button to say “Tell me” when it’s idle, then “One sec…” while it’s loading. I also like to assign the aria-disabled attribute to match the isRunning state. This will hint to assistive technology that it’s not ready to be clicked (though technically still can be). It can also be targeted with CSS to provide visual styles suggesting it’s not quite ready to be clicked again.', '', 'Ok, we’ve done way too much work without actually seeing the results on the page. It’s time to change that. Let’s bring the fetch request we prototyped earlier in the browser into our application.', 'We can copy/paste the fetch code right into the body of our action handler, but to access the user’s input data, we’ll need access to the form data that is submitted. Fortunately, any data passed to the action.submit() method will be available to the action handler as the first parameter. It will be a serialized object where the keys correspond to the form control names.', 'Note that I’ll be using the await keyword in the body of the handler, which means I also have to tag the handler as an async function.', '', 'At the end of the action handler, we also want to return some data for the front end. The OpenAI response comes back as JSON, but I think we might as well just return the text. If you remember from the response object we saw above, that data is located at responseBody.choices[0].message.content.', 'If we set things up correctly, we should be able to access the action handler’s response in the ActionStore‘s value property. This means we can conditionally render it somewhere in the template like so:', '', 'Alright, we’ve moved the OpenAI request to the backend, and protected our API keys from prying eyes, we’re getting a (mediocre joke) response, and displaying it on the front end. The app is working, but there’s still one more security issue to deal with.', 'It’s generally a bad idea to hardcode API keys into your source code, for some reasons:', 'A better system is to use environment variables. With environment variables, you can provide the API keys only to the systems and users that need access to them.', 'For example, you can make an environment variable called OPENAI_API_KEY with the value of your OpenAI key for only the production environment. This way, only developers with direct access to that environment would be able to access it. This greatly reduces the likelihood of the API keys leaking, it makes it easier to share your code openly, and because you are limiting access to the keys to the least number of people, you don’t need to replace keys as often because someone left the company.', 'In Node.js, it’s common to set environment variables from the command line (ENV_VAR=example npm start) or with the popular dotenv package. Then, in your server-side code, you can access environment variables using process.env.ENV_VAR.', 'Things work slightly differently with Qwik.', 'Qwik can target different JavaScript runtimes (not just Node), and accessing environment variables via process.env is a Node-specific concept. To make things more runtime-agnostic, Qwik provides access to environment variables through an RequestEvent object which is available as the second parameter to the route action handler function.', '', 'So that’s how we access environment variables, but how do we set them?', 'Unfortunately, for production environments, setting environment variables will differ depending on the platform. For a standard server VPS, you can still set them with the terminal as you would in Node (ENV_VAR=example npm start).', 'In development, we can alternatively create a local.env file containing our environment variables, and they will be automatically assigned to us. This is convenient since we spend a lot more time starting the development environment, and it means we can provide the appropriate API keys only to the people who need them.', 'So after you create a local.env file, you can assign the OPENAI_API_KEY variable to your API key.', '', '(You may need to restart your dev server)', 'Then we can access the environment variable through the RequestEvent parameter. With that, we can replace the hard-coded value in our fetch request’s Authorization header with the variable using Template Literals.', '', 'For more details on environment variables in Qwik, see their documentation.', 'Here’s what my final component looks like, including some Tailwind classes and a slightly different template.', '', 'All right! We’ve gone from a script that uses AI to get mediocre jokes to a full-blown application that securely makes HTTP requests to a backend that uses AI to get mediocre jokes and sends them back to the front end to put those mediocre jokes on a page.', 'You should feel pretty good about yourself.', 'But not too good, because there’s still room to improve.', 'In our application, we are sending a request and getting an AI response, but we are waiting for the entirety of the body of that response to be generated before showing it to the users. These AI responses can take a while to complete.', 'If you’ve used AI chat tools in the past, you may be familiar with the experience where it looks like it’s typing the responses to you, one word at a time, as they’re being generated. This doesn’t speed up the total request time, but it does get some information back to the user much sooner and feels like a faster experience.', 'In the next post, we’ll learn how to build that same feature using HTTP streams, which are fascinating and powerful but also can be kind of confusing. So I’m going to dedicate an entire post just to that.', 'I hope you’re enjoying this series and plan to stick around. In the meantime, have fun generating some mediocre jokes.', 'Thank you so much for reading. If you liked this article, and want to support me, the best ways to do so are to share it and follow me on Twitter.']"
How To Use LangChain4j With LocalAI,https://dzone.com/articles/how-to-use-langchain4j-with-localai,"['In this post, you will learn how you can integrate Large Language Model (LLM) capabilities into your Java application. More specifically, how you can integrate with LocalAI from your Java application. Enjoy!', 'In a previous post, it was shown how you could run a Large Language Model (LLM) similar to OpenAI by means of LocalAI. The Rest API of OpenAI was used in order to interact with LocalAI. Integrating these capabilities within your Java application can be cumbersome. However, since the introduction of LangChain4j, this has become much easier to do. LangChain4j offers you a simplification in order to integrate with LLMs. It is based on the Python library LangChain. It is therefore also advised to read the documentation and concepts of LangChain since the documentation of LangChain4j is rather short. Many examples are provided though in the LangChain4j examples repository. Especially, the examples in the other-examples directory have been used as inspiration for this blog.', 'The real trigger for writing this blog was the talk I attended about LangChain4j at Devoxx Belgium. This was the most interesting talk I attended at Devoxx: do watch it if you can make time for it. It takes only 50 minutes.', 'The sources used in this blog can be found on GitHub.', 'The prerequisites for this blog are:', 'In this section, some of the capabilities of LangChain4j are shown by means of examples. Some of the examples used in the previous post are now implemented using LangChain4j instead of using curl.', 'As a first simple example, you ask the model how it is feeling.', 'In order to make use of LangChain4j in combination with LocalAI, you add the langchain4j-local-ai dependency to the pom file.', '', 'In order to integrate with LocalAI, you create a ChatLanguageModel specifying the following items:', 'Next, you ask the model to generate an answer to your question and you print the answer.', '', 'Start LocalAI and run the example above.', 'The response is as expected.', '', 'Before continuing, note something about the difference between LanguageModel and ChatLanguageModel. Both classes are available in LangChain4j, so which one to choose? A chat model is a variation of a language model. If you need a ""text in, text out"" functionality, you can choose LanguageModel. If you also want to be able to use ""chat messages"" as input and output, you should use ChatLanguageModel.', 'In the example above, you could just have used LanguageModel and it would behave similarly.', 'Let’s verify whether it also returns facts about the famous Dutch soccer player Johan Cruijff. You use the same code as before, only now you set the temperature to zero because no creative answer is required.', '', 'Run the example, the response is as expected.', '', 'Sometimes, the answer will take some time. In the OpenAPI specification, you can set the stream parameter to true in order to retrieve the response character by character. This way, you can display the response already to the user before awaiting the complete response.', 'This functionality is also available with LangChain4j but requires the use of a StreamingResponseHandler. The onNext method receives every character one by one. The complete response is gathered in the answerBuilder and futureAnswer. Running this example prints every single character one by one, and at the end, the complete response is printed.', '', 'Run the example. The response is as expected.', '', 'You can instruct the model by means of a system message how it should behave. For example, you can instruct it to answer always in a different language; Dutch, in this case. This example shows clearly the difference between LanguageModel and ChatLanguageModel. You have to use ChatLanguageModel in this case because you need to interact by means of chat messages with the model.', 'Create a SystemMessage to instruct the model. Create a UserMessage for your question. Add them to a list and send the list of messages to the model. Also, note that the response is an AiMessage.', 'The messages are explained as follows:', '', 'Run the example, the response is as expected.', '', 'A fantastic use case is to use an LLM in order to chat with your own documents. You can provide the LLM with your documents and ask questions about it.', 'For example, when you ask the LLM for which football clubs Johan Cruijff played (""For which football teams did Johan Cruijff play and also give the periods, answer briefly""), you receive the following answer.', '', 'This answer is quite ok, but it is not complete, as not all football clubs are mentioned and the period for Ajax includes also his youth period. The correct answer should be:', 'Apparently, the LLM does not have all relevant information and that is not a surprise. The LLM has some basic knowledge, it runs locally and has its limitations. But what if you could provide the LLM with extra information in order that it can give an adequate answer? Let’s see how this works.', 'First, you need to add some extra dependencies to the pom file:', '', 'Save the Wikipedia text of Johan Cruijff to a PDF file and store it in src/main/resources/example-files/Johan_Cruyff.pdf. The source code to add this document to the LLM consists of the following parts:', '', 'When you execute this code, an exception is thrown.', '', 'This can be solved by setting the timeout of the language model to a higher value.', '', 'Run the code again, and the following answer is received, which is correct.', '', 'Using a 1.x version of LocalAI gave this response, which was worse.', '', 'The following steps were used to solve this problem.', 'When you take a closer look at the PDF file, you notice that the information about the football teams is listed in a table next to the regular text. Remember that splitting the document was done by creating chunks of 500 characters. So, maybe this splitting is not executed well enough for the LLM.', '', 'Copy the football teams in a separate text document.', '', 'Add both documents to the ingestor.', '', 'Run this code and this time, the answer was correct and complete.', '', 'It is therefore important that the sources you provide to an LLM are split wisely. Besides that, the used technologies improve in a rapid way. Even while writing this blog, some problems were solved in a couple of weeks. Updating to a more recent version of LocalAI for example, solved one way or the other the problem with parsing the single PDF.', 'In this post, you learned how to integrate an LLM from within your Java application using LangChain4j. You also learned how to chat with documents, which is a fantastic use case! It is also important to regularly update to newer versions as the development of these AI technologies improves continuously.']"
Getting Started With Spring AI and PostgreSQL PGVector,https://dzone.com/articles/spring-ai-with-postgresql-pgvector,"['The Spring AI is a new project of the Spring ecosystem that streamlines the creation of AI applications in Java. By using Spring AI together with PostgreSQL pgvector, you can build generative AI applications that draw insights from your data.', 'First, this article introduces you to the Spring AI ChatClient that uses the OpenAI GPT-4 model to generate recommendations based on user prompts. Next, the article shows how to deploy\xa0PostgreSQL with the PGVector extension and perform vector similarity searches using the Spring AI EmbeddingClient and Spring JdbcClient.', 'Spring AI supports many large language model\xa0(LLM) providers, with each LLM having its own Spring AI dependency.', ""Let's assume that you prefer working with OpenAI models and APIs. Then, you need to add the following dependency to a project:"", '', ""Also, at the time of writing, Spring AI was in active development, with the framework artifacts being released in the Spring Milestone and/or Snapshot repositories. Thus, if you still can't find Spring AI on https://start.spring.io/, then add the repositories to the pom.xml file:"", 'The OpenAI module comes with several configuration properties, allowing the management of connectivity-related settings and fine-tuning the behavior of OpenAI models.', 'At a minimum, you need to provide your OpenAI API key, which will be used by Spring AI to access GPT and embedding models. Once the key is created, add it to the application.properties file:', '', 'Then, if necessary, you can select particular GPT and embedding models:', '', ""In the end, you can test that the OpenAI module is configured properly by implementing a simple assistant with Spring AI's ChatClient:"", '', 'For the sake of the experiment, if you pass the ""I\'d like to stay near the Golden Gate Bridge""\xa0prompt, then the searchPlaces the method might provide lodging recommendations as follows:', ""If you run the previous code snippet with the ChatClient, you'll notice that it usually takes over 10 seconds for the OpenAI GPT model to generate a response. The model has a broad and deep knowledge base, and it takes time to produce a relevant response."", 'Apart from the high latency, the GPT model might not have been trained on data that is relevant to your application workload. Thus, it might generate responses that are far from being satisfactory for the user.', 'However, you can always expedite the search and provide users with accurate responses if you generate embeddings on a subset of your data and then let Postgres work with those embeddings.', 'The pgvector extension allows storing and querying vector embeddings in Postgres. The easiest way to start with PGVector is by starting a Postgres instance with the extension in Docker:', '', 'Once started, you can connect to the container and enable the extension by executing the CREATE EXTENSION vector statement:\xa0', '', '\xa0Lastly, add the Postgres JDBC driver dependency to the pom.xml file:', '', 'Configure the Spring DataSource by adding the following settings to the application.properties file:', 'At a minimum, the vector similarity search is a two-step process.', ""First, you need to use an embedding model to generate a vector/embedding for a provided user prompt or other text. Spring AI supports the EmbeddingClient that connects to OpenAI's or other providers' embedding models and generates a vectorized representation for the text input:"", '', 'Second, you use the generated embedding to perform a similarity search across vectors stored in the Postgres database. For instance, you can use the Spring JdbcClient for this task:', '', ""Spring AI and PostgreSQL PGVector provide all the essential capabilities needed for building generative AI applications in Java. If you're curious to learn more, watch this hands-on tutorial. It guides you through the process of creating a lodging recommendation service in Java from scratch, optimizing similarity searches with specialized indexes, and scaling with distributed Postgres (YugabyteDB):"", '']"
Protecting Privacy in the Age of Edge AI: The Role of Homomorphic Encryption,https://dzone.com/articles/protecting-privacy-in-the-age-of-edge-ai-the-role,"['Recent years have witnessed a mounting concern about data privacy, and these concerns are not unfounded. In a world where connectivity is ubiquitous, the statistics paint a compelling picture. According to a report by Cisco, the number of connected devices worldwide is projected to reach a staggering 29.3 billion by 2023. This exponential growth for the Internet of Things (IoT) devices underscores the urgent need for robust privacy measures.', 'Furthermore, a survey conducted by the Pew Research Center has revealed that a significant 79% of Americans express concern about the way their data is being utilized by companies. This growing awareness among users regarding their digital privacy signifies a shifting paradigm where individuals are increasingly vigilant about safeguarding their personal information.', 'In tandem with the rising tide of privacy concerns, the adoption of Edge AI has surged. In 2021, the edge AI market was valued at USD 11.98 billion, and forecasts suggest it will reach an impressive USD 107.47 billion by 2029, according to Fortune Business Insights. This rapid expansion underscores the relevance and significance of Edge AI in the ever-evolving landscape of artificial intelligence.', ""Gartner, in its 2023 Hype Cycle for Artificial Intelligence, predicts that Edge AI is poised to enter the mainstream within the next two years. This prediction reaffirms the technology's growing prominence and its pivotal role in addressing the privacy concerns of the digital age."", 'In response to these escalating concerns and the burgeoning market demands for privacy-centric solutions, tech industry titans like Apple have taken a proactive stance. They have embraced innovative approaches designed to prioritize user privacy without sacrificing advanced functionalities.', 'Apple, for instance, has introduced features such as ""on-device processing"" and ""differential privacy."" These features ensure that user data remains exclusively on the device, bolstering privacy safeguards while still offering cutting-edge functionality.', ""Similarly, Google has committed to developing privacy-preserving machine learning techniques. This commitment underscores the tech industry's collective shift towards privacy-conscious AI solutions, heralding a future where technology not only empowers users but also safeguards their personal data."", 'Some examples of privacy-preserving techniques used in edge AI include:', 'Homomorphic Encryption is a revolutionary technology that stands as a guardian of privacy in the realm of Edge AI. It allows computations to be performed on encrypted data without the need for decryption. In an age where data privacy and security are paramount, Homomorphic Encryption emerges as a formidable solution, particularly at the edge.', 'At its core, Homomorphic Encryption enables secure data processing without ever exposing sensitive information. This is a game-changer in scenarios where preserving data privacy is non-negotiable, such as healthcare, finance, and personal devices. Imagine a wearable healthcare device equipped with Edge AI capabilities. With Homomorphic Encryption, it can process your health data locally without the need to reveal the raw information. This ensures that your private health information remains confidential, even during data analysis.', ""The practical implications of Homomorphic Encryption are profound. In the financial sector, sensitive transactions can be securely processed at the edge, guaranteeing confidentiality while harnessing the power of AI. In personal devices like smartphones, your data can be analyzed without leaving the device, preserving your privacy at all times. It's a remarkable blend of AI capabilities and data security that epitomizes the essence of Edge AI."", 'As we look ahead, Edge AI is poised to redefine the boundaries of technology and privacy. It promises an era where intelligent decision-making is not just swift but also profoundly secure. With data processed locally, users regain control over their information, ensuring that privacy is not just an option but a fundamental right.', 'In this intelligent, efficient, and secure future, sectors like autonomous vehicles, healthcare, and the Internet of Things (IoT) flourish. Energy is optimized, traffic flows smoothly, and emergency responses are swift and precise. Augmented reality transforms education and gaming while healthcare embraces personalized monitoring. Edge AI becomes a beacon of sustainability, conserving energy and driving environmental conservation efforts.', 'Edge AI represents a monumental shift in the AI landscape — one that champions both innovation and discretion. It signifies a future where technology is seamlessly integrated into our daily lives, respecting the sanctity of our privacy. In the interconnected world we inhabit, Edge AI stands as a sentinel, ensuring that in the age of AI, privacy remains an unwavering cornerstone. It is not just a technological advancement but a societal assurance — a promise that privacy is not a trade-off for progress but a bedrock upon which innovation thrives.', 'As users become more conscious of their digital footprint, the adoption of Edge AI is not just a technological evolution; it is a societal affirmation that privacy is not an afterthought but a fundamental right. In a world powered by Edge AI, intelligence knows no bounds, and privacy is inviolable.']"
5 Hard Truths About Generative AI for Technology Leaders,https://dzone.com/articles/5-hard-truths-about-generative-ai-for-technology-l,"['GenAI is everywhere you look, and organizations across industries are putting pressure on their teams to join the race – 77% of business leaders fear they’re already missing out on the benefits of GenAI.', 'Data teams are scrambling to answer the call. But building a generative AI model that actually drives business value is\xa0hard.', 'And in the long run, a quick integration with the OpenAI API won’t cut it. It’s GenAI, but where’s the moat? Why should users pick you over ChatGPT?', 'That quick check of the box feels like a step forward. Still, if you aren’t already thinking about how to connect LLMs with your proprietary data and business context actually to drive differentiated value, you’re behind.\xa0', 'That’s not hyperbole. This week, I’ve talked with half a dozen data leaders on this topic alone. It wasn’t lost on any of them that this is a race. At the finish line, there are going to be winners and losers: the Blockbusters and the Netflixes.', 'If you feel like the starter’s gun has gone off, but your team is still at the starting line stretching and chatting about “bubbles” and “hype,” I’ve rounded up five hard truths to help shake off the complacency.', '“Barr, if generative AI is so important, why are the current features we’ve implemented so poorly adopted?”', 'Well, there are a few reasons. One, your AI initiative wasn’t built to respond to an influx of well-defined user problems. For most data teams, that’s because you’re racing, and it’s early, and you want to gain some experience. \xa0However, it won’t be long before your users have a problem that GenAI best solves, and when that happens – you will have much better adoption compared to your tiger team brainstorming ways to tie GenAI to a use case.', 'And because it’s early, the generative AI features that have been integrated are just “ChatGPT but over here.”\xa0', 'Let me give you an example. Think about a productivity application you might use every day to share organizational knowledge. An app like this might offer a feature to execute commands like “Summarize this,” “Make longer,” or “Change tone” on blocks of unstructured text. One command equals one AI credit.\xa0', 'Yes, that’s helpful, but it’s not differentiated.\xa0', 'Maybe the team decides to buy some AI credits, or perhaps they just simply click over on the other tab and ask ChatGPT. I don’t want to completely overlook or discount the benefit of not exposing proprietary data to ChatGPT. Still, it’s also a smaller solution and vision than what’s being painted on earnings calls across the country.\xa0', '', 'So consider: What’s your GenAI differentiator and value add? Let me give you a hint: high-quality proprietary data.', ""That’s why a RAG model (or sometimes, a fine-tuned model) is so important for Gen AI initiatives. It gives the LLM access to that enterprise's proprietary data. (I’ll explain why below.)"", 'It’s true: generative AI is intimidating.\xa0', 'Sure, you could integrate your AI model more deeply into your organization’s processes, but that feels risky. Let’s face it: ChatGPT hallucinates and can’t be predicted. There’s a knowledge cutoff that leaves users susceptible to out-of-date output. There are legal repercussions to data mishandling and providing consumers with misinformation, even if accidental.', '', 'Your data mishaps have consequences. And that’s why it’s essential to know exactly what you are feeding GenAI and that the data is accurate.\xa0', 'In an anonymous survey, we sent to data leaders asking how far away their team is from enabling a Gen AI use case, one response was, “I don’t think our infrastructure is the thing holding us back. We’re treading quite cautiously here – with the landscape moving so fast and the risk of reputational damage from a ‘rogue’ chatbot, we’re holding fire and waiting for the hype to die down a bit!”\xa0', 'This is a widely shared sentiment across many data leaders I speak to. If the data team has suddenly surfaced customer-facing, secure data, then they’re on the hook. Data governance is a massive consideration and a high bar to clear.\xa0', 'These are real risks that need solutions, but you won’t solve them by sitting on the sideline. There is also a real risk of watching your business being fundamentally disrupted by the team that figured it out first.\xa0', 'Grounding LLMs in your proprietary data with fine tuning and RAG is a big piece to this puzzle, but it’s not easy…', 'I believe that RAG (retrieval augmented generation) and fine-tuning are the centerpieces of the future of enterprise generative AI. However, RAG is a simpler approach in most cases; developing RAG apps can still be complex.', '', 'Can’t we all just start RAGing? What’s the big deal?\xa0', 'RAG might seem like the obvious solution for customizing your LLM. But RAG development comes with a learning curve, even for your most talented data engineers. They need to know prompt engineering, vector databases and embedding vectors, data modeling, data orchestration, data pipelines, and all for RAG. And, because it’s new (introduced by Meta AI in 2020), many companies just don’t yet have enough experience with it to establish best practices.\xa0', 'RAG implementation architecture', 'Here’s an oversimplification of RAG application architecture:\xa0', 'There are a lot of complexities in this architecture, but it does have important benefits:', 'We can see this becoming a reality in the Modern Data Stack. The biggest players are working at a breakneck speed to make RAG easier by serving LLMs within their environments, where enterprise data is stored. Snowflake Cortex now enables organizations to analyze data and build AI apps directly in Snowflake quickly. Databricks’ new Foundation Model APIs provide instant access to LLMs directly within Databricks. Microsoft released Microsoft Azure OpenAI Service, and Amazon recently launched the Amazon Redshift Query Editor.\xa0', 'Snowflake data cloud', 'I believe all of these features have a good chance of driving high adoption. But, they also heighten the focus on data quality in these data stores. If the data feeding your RAG pipeline is anomalous, outdated, or otherwise untrustworthy data, what’s the future of your generative AI initiative?', 'Take a good, hard look at your data infrastructure. Chances are, if you had a perfect RAG pipeline, fine-tuned model, and clear use case ready to go tomorrow (and wouldn’t that be nice?), you still wouldn’t have clean, well-modeled datasets to plug it all into.\xa0', 'Let’s say you want your chatbot to interface with a customer. To do anything useful, it needs to know about that organization’s relationship with the customer. If you’re an enterprise organization today, that relationship is likely defined across 150 data sources and five siloed databases…3 of which are still on-prem.\xa0', 'If that describes your organization, it’s possible you are a year (or two!) away from your data infrastructure being GenAI-ready.\xa0', 'This means if you want the option to do something with GenAI someday soon, you need to be creating useful, highly reliable, consolidated, well-documented datasets in a modern data platform… yesterday. Or the coach will call you into the game, and your pants will be down.', 'Your data engineering team is the backbone for ensuring data health. A modern data stack enables the data engineering team to monitor data quality continuously in the future.\xa0', 'It’s 2024 now. Launching a website, application, or any data product without data observability is a risk. Your data\xa0is a product, requiring data observability and governance to pinpoint data discrepancies before they move through an RAG pipeline.\xa0', 'Generative AI is a team sport, especially when it comes to development. Many data teams make the mistake of excluding key players from their Gen AI tiger teams, and that’s costing them in the long run.', 'Who should be on an AI tiger team? Leadership, or a primary business stakeholder, to spearhead the initiative and remind the group of the business value. Software engineers will develop the code, the user-facing application, and the API calls. Data scientists consider new use cases, fine-tune their models, and push the team in new directions. Who’s missing here?', 'Data engineers.\xa0', 'Data engineers are critical to Gen AI initiatives. They will be able to understand the proprietary business data that provides the competitive advantage over a ChatGPT, and they will build the pipelines that make that data available to the LLM via RAG.\xa0', 'If your data engineers aren’t in the room, your tiger team is not at full strength. The most pioneering companies in GenAI are telling me they are already embedding data engineers in all development squads.\xa0', 'If any of these hard truths apply to you, don’t worry. Generative AI is in such nascent stages that there’s still time to start back over and, this time, embrace the challenge.\xa0', 'Take a step back to understand the customer needs an AI model can solve, bring data engineers into earlier development stages to secure a competitive edge from the start, and take the time to build a RAG pipeline that can supply a steady stream of high-quality, reliable data.\xa0', 'And invest in a modern data stack. Tools like data observability will be a core component of data quality best practices – and generative AI without high-quality data is just a whole lot of fluff. ']"
Search for Rail Defects (Part 3),https://dzone.com/articles/analysis-of-the-possibility-of-building-an-effecti,"['To ensure the safety of rail traffic, non-destructive testing of rails is regularly carried out using various approaches and methods. One of the main approaches to determining the operational condition of railway rails is ultrasonic non-destructive testing. The assessment of the test results depends on the defectoscopist. The need to reduce the workload on humans and improve the efficiency of the process of analyzing ultrasonic testing data makes the task of creating an automated system relevant. The purpose of this work is to evaluate the possibility of creating an effective system for recognizing rail defects from ultrasonic inspection defectograms using ML methods.', 'The railway track consists of rail sections connected together by bolts and welded joints. When a defectoscope device equipped with generating piezoelectric transducers (PZTs) passes along the railway track, ultrasonic pulses are emitted into the rail at a predetermined frequency. The receiving PZTs then register the reflected waves. The detectability of defects by the ultrasonic method is based on the principle of reflection of waves from inhomogeneities in the metal since cracks, including other inhomogeneities, differ in their acoustic resistance from the rest of the metal.', 'The registered signal reflected from a bolt hole with a perpendicular input of the probing pulse to the rail surface is presented in Figure 1. The image of such a signal is called «Amplitude scan» or abbreviated «A-scan».', '', 'Fig. 1: Presentation of the registered ultrasonic inspection signal on A-scan: a) ultrasound emission and registration process, b) registered signal.', 'The recorded amplitude of such an echo signal at each i coordinate along the length of the rail can be represented as a vector. Ai = [a1, a2, a3, ... , \xa0a j\xa0], \xa0where aj\xa0- is the amplitude of the reflected signal at the j-th depth level of the rail. The depth for each amplitude value aj is calculated based on the registration time and the frequency of the emitted signal.', 'The recorded A-scan echo signals at each i inspection point along the length of the rail can be represented as a two-dimensional array. B = [A1, A2, A3, ... , \xa0Ai] of size \xa0(i\xa0x\xa0j). Figure 2 schematically shows a fragment of array B with the recorded echo signals reflected from a bolt hole with a perpendicular input of the probing pulse to the surface of the rail.', '', '', 'Fig. 2: Fragment of the array with the signals of the bolt hole and the bottom signal.\xa0', 'The graphical representation of the two-dimensional array B in the form of an intensity graph is called «Bright-scan» («B-scan»), Figure 3, while the values of the array are displayed in three dimensions on a plane by using color as the third dimension of the data along the Z-axis.', '', 'Fig. 3: Fragment of the B-scan of a bolt hole obtained by scanning with a perpendicular input of the probing pulse to the surface of the rail (Avikon-11 equipment).', 'The different reflective properties of defects, their geometry, and their location in the rail require the use of ultrasonic transducers with different angles of input and registration for their detection. Therefore, modern rail flaw detectors use several transducers that are distributed along the length of the flaw detector search system and form a so-called rail section sounding scheme. One of the applied inspection schemes is shown in Figure 4, where the generating and registering transducers of each angle of ultrasonic input are located in the same housing.', '', 'Fig. 4: Example of a scheme for emitting ultrasonic pulses into a rail using six transducers', 'The formation of B-scan signals for a bolt hole using transducers with central input angles of «+420» (orange), « - 420» (blue), and «00» (green) at three characteristic points (1, 2, 3 positions) along the length of the rail is shown schematically in Figure 5a.', '', 'Fig. 5: Signal formation during scanning: a) general view b) correction with offset.', 'The information channels of a flaw detector correspond to physical sensors (transducers) that are sequentially arranged on the surface of the rail. The set of B-scans for all channels of a flaw detector for each rail, combined into a data file, is called a defectogram (scan). Often, a channel or a set of channels selected for consideration is also called a defectogram.', 'In most cases, to improve the perception of the defectogram, it is displayed in the mode of reducing to a single section, in which the coordinates of the echo signals for channels with an inclined input of ultrasound are corrected by additionally taking into account the distance of the reflector from the point of input of the probing pulse into the metal of the rail (Figure 5b). In addition, for ease of use and reduction of the graphic appearance of the entire defectogram, a graphical grouping of data channels is performed, one of which is shown in Figure 6.', '', 'Fig. 6: An example of a section of a defectogram of a bolted rail joint obtained by scanning with ultrasonic equipment Avicon-11.', 'To visually search for defects on the B-scan and A-scan, the cognitive functions of attracted experts — flaw detectors — are used. When ultrasonic scanning of rails, their structural elements and defects have acoustic responses, which are displayed on the defectogram in the form of characteristic graphic images. Each type of defect on the defectogram is visually distinguishable for experts during the data analysis process. The main goal of defectogram analysis is to reliably find and highlight graphic images of defects against the background of possible interference and images of structural elements.', 'Each measuring channel of the defectogram 00,\xa0±420,\xa0±580, +700 or their combination is designed to detect a specific group of defects. To simplify the task of searching for defects, we will decompose the problem and consider the capabilities of DL algorithms for searching for individual types of defective areas using the defectogram of channel \xa0«00» of the Avicon-11 flaw detector. In this case, the types of sites can be divided into four classes based on characteristic information features. Some idea of the diversity of the data set obtained by the Avicon-11 flaw detector can be obtained from Table 1.\xa0', '', 'Table 1: Examples of instances (B-scan) for selected classes (real data)', 'Despite the fact that in the operation of the railway track, the presence or absence of a defect (binary classification) is decisive, we will quantitatively assess which defective areas have a high probability of being falsely classified as non-defective, which is a dangerous case in rail diagnostics. In this work, the classification task is reduced to an unambiguous multi-class task with four classes.', 'The data set is collected from defectograms obtained by the Avicon-11 flaw detector on several Railroad Test Tracks (RTT) and conventional tracks under various conditions. Each data instance is represented as rectangular ""depth × long"" data and has the shape (224, 1024), which allows you to fit images of more than six bolt holes along the length of the rail at their bolted joint.', 'The formation of a data set is difficult due to the lack of a sufficient number of defective areas, so to expand it, we used a displacement along the length of the rail and scanning of the same defect under different conditions and test equipment settings, which allows us to obtain different images of defects (Fig. 7).', '', 'Fig. 7: Example of dataset expansion', 'As a result of the specified methodology, the dataset for classes 0, 1, 2, and 3 is 2151, 1043, 1584, and 582, respectively, for a total of 5360 instances. The defect-free class «0» contains 10% (214 instances) of instances without bolt holes, and the remaining 90% (1937 instances) contain from one to six bolt holes. The dataset is named ""avicon"" and is used only for final testing. This allows us to avoid the problem of class imbalance during training and to obtain a more reliable assessment of the accuracy of the classifier.\xa0', 'For the purposes of training and testing classification models in this work, a synthetic, balanced dataset is used, obtained on the basis of mathematical modeling of models describing the process of reflection and registration of ultrasonic waves from structural reflectors of rails and defects. The application of such a trained model for the classification of real data obtained by a flaw detector during rail diagnostics is demonstrated in Fig. 8.', '', 'Fig. 8: Application of a neural network trained on model data', 'Examples of synthetic instances of the selected classes are presented in Table 2. For more information on the generation of synthetic datasets, please see the works [1-4].', '', '', 'The modeling process allows us to obtain a significant number of instances; we will limit our work to 2048 instances for each of the synthetic sets «train», «valid», «test».', 'Each instance of data and label is written for each set in the corresponding binary files images.bin and labels.bin (data type «uint8») according to Fig. 9.', '', 'Fig. 9: Distribution of sets by directory', 'Information on the amount of data, class balance for synthetic sets, and the «avicon» set is presented in Fig. 10.', '', 'Analysis of the graphical representation of frames of real data allows us to identify at least one important property of class 3 defects: the images of defects are most difficult to distinguish from the images of bolt holes, especially if they are at the same level as the rail depth, which significantly complicates the classification task.', 'Each data instance is 224 x 1024 in size, which is large enough for the application of machine learning (ML) algorithms but also causes difficulties in organizing the training process. Each such instance can be considered as data points in a 224*1024 = 229376-dimensional space, which is highly sparse because it contains a large number of zeros. The constructed graph of the integral explainable dispersion of the \xa0«train» set as a function of the number of components of the PCA method (Fig. 11) shows that when using 1000 components (330 times smaller than the original size) already 98.5% of the dispersion is explained, which indicates a high level of redundancy in the original data. Such a reduced dataset can be used in ML algorithms, but obtaining it on the entire dataset at the same time causes difficulties; therefore, further in the work, an algorithm based on Deep Learning is considered.', '', 'Fig. 11: Graph of the integral explainable dispersion of the data as a function of the number of components of the PCA method for the «train» dataset', 'In the work, a DL model in the form of a linear stack of layers is considered (Fig. 12a: the final version of the network). Activation function: relu (rectified linear unit), for the output fully connected layer — normalized exponential softmax function, with the sum of the values of all output neurons equal to one. Loss function: a measure of error in the form of the distance between the probability distributions of actual data and their forecast (cross-entropy). Optimizer: stochastic gradient descent algorithm in the RMSProp modification. Metrics in the training process: accuracy, as a value equal to the ratio of the number of correctly classified objects to the total number of objects.', 'The final version of the network was trained for 50 epochs. The graphs of the changes in the quality indicators «loss» and «accuracy» characterizing the training process (Fig. 12b) converge at the training and validation stages and have low and high values, respectively, which may indicate the absence of the overfitting effect of the model. This fact is also confirmed by the relative equality of the obtained prediction accuracies of the model on the «train» set - 99.61% and the «test» set - 99.02% (Fig. 12b,c). The memory occupied by the network in H5 format is 30 KB. The full code can be found in the GitHub repository at the link [5].', '', 'Fig. 12: NN and the results of its training: a) network architecture; b) Change in «Loss» and «Accuracy» during training; c) Classification Report; d) Confusion matrix', 'The confusion matrix and the classification report are presented in Figure 12d,c. The trained model has high precision and recall scores above 96% for all class classifiers, which also means that there are sufficient information features in the data for classification.', 'It is important to consider the misclassified samples to understand the operation of the classifier and its changes. According to the confusion matrix, the classifier of class 3 incorrectly recognized four samples of class 0 that have at least one bolt-hole signal similar to the image of a defect of group 3 (an example is shown in Fig. 13a), which may have been the cause of the error.', 'The class 0 classifier incorrectly recognized two samples of class 1. Both incorrectly recognized defects have a characteristic appearance and are located very close to the upper boundary of the data frame. One of such frames is shown in Fig. 13b.', 'The class 0 classifier incorrectly recognized one sample of class 2, which is located close enough to the depth of the bolt holes (Fig. 13c).', 'The class 0 classifier incorrectly recognized 13 samples of class 3, which is located close enough to the depth of the bolt holes (Fig. 13d).', 'The results of the network tests indicate the difficulty of distinguishing a class 3 defect from bolt holes.', '', 'Fig. 13: Characteristic frames of incorrectly classified data', 'To assess the quality of the work of the trained neural network for recognizing instances of real data obtained by the Avicon-11 flaw detector, modeling was performed on the avicon dataset. The accuracy of the entire network was 90%, which is 9% lower than the prediction accuracy for synthetic data. The resulting confusion matrix and summary report on the quality of the model are presented in Fig. 14. The time required to classify the tagged data makes it possible to estimate the time required to classify 100 km of railway line — 11 s.', '', '', 'Fig. 14: Summary report on the quality of the model based on the classification of the «avicon» dataset', 'We will analyze the most important data classification errors. According to the confusion matrix (Fig. 14), four transverse cracks with the weakest response recorded by channel «00» \xa0and belonging to class 1 were classified as class 0 (without defect). To improve the recognition of such defects, it is necessary to add additional information features that can be obtained from the inclined channels of the flaw detector, which are the main channels for detecting such types of defects (Fig. 15).', '', 'Fig. 15: Typical frames of misclassified data (True = 1, Predict = 0))', 'The incorrect classification of 49 class 2 defects as non-defective is associated with weak signal responses recorded by the measuring channel «00». One way to improve the classification of such class 2 samples is to consider additional information features from the inclined channels (Ch ±420), as they are the main channels for detecting incorrectly classified defects (Fig. 16).', '', 'Fig. 16: Typical misclassified data frame of class 2 (True =2, Predict = 0)', 'The misclassification of 112 class 3 samples to class 0 is associated with data frames where the defect image is:', 'The misclassification of 152 class 0 samples into class 3 samples is due to a similar reason — the similarity of bolt-hole patterns to class 3 defect patterns.\xa0', 'One of the ways to improve the classification of samples of classes 0 and 3 is to consider additional information signs of inclined channels (Ch ±420), since in this case the bolt holes are well distinguished from a defect of class 3 and vice versa (Fig. 17).', '', 'Fig. 17: Typical misclassified data frame of class 3 (True =3, Predict = 0)\xa0', 'The graphical images of defects of classes 1 and 2 are similar, and the assignment of a defect to class 1 or 2 depends on the depth from which the image of the defect begins to be recorded on the defectogram. Defects of class 1 are located in the head of the rail. Defects of class 2 can be recorded starting from the transition zone of the rail from its head to the neck zone. The incorrect classification of 165 defects of class 1 as defects of class 2 is most likely associated with weak defect responses recorded in the head of the rail (Fig. 18).', '', 'Fig. 18: Typical misclassified data frames of class 1 (True =1, Predict = 2)', 'One of the important tasks of the obtained classifier in its practical use will be the accurate definition of the non-defective class (class 0), which will allow the exclusion of the false assignment of defective samples to non-defective ones. It is possible to reduce the number of false positives for the class 0 classifier by changing the probability cutoff threshold. To evaluate the applicable threshold level of cutoff, the multiclass task was binarized with the isolation of the non-defective state and all defective states, which corresponds to the «one versus rest» strategy (One vs Rest). By default, for binary classification, the threshold value is taken to be 0.5 (50%). With this approach, the binary classifier has an accuracy of 92.28% (Fig. 19).', '', 'Fig. 19: Qualitative indicators of a binary classifier at a cutoff threshold of 0.5 («avicon» set)', 'The changes in precision and recall of the binary classifier depending on the changing threshold value are presented in a «precision-recall curve» graph (Fig. 20a). With a threshold value of 0.5, the value of false positives is 161 samples (Fig. 20b). Increasing the threshold value to 0.8, and 0.9 allows to reduce the number of false positives to 70 and 58, respectively, due to an increase in false negatives to 344 and 440 (Fig. 20b).\xa0', 'It can be said that in automatic analysis, increasing the threshold value allows, on the one hand, to reduce the false assignment of defects to the non-defective state and thereby reduce the risk of missing defects. On the other hand, increases the labor intensity of a person during manual analysis of frames with known defects.', '', '', 'Fig. 20: Influence of the cutoff threshold on the characteristics of a binary classifier: a) precision-recall curve, b) confusion matrix at different cutoff thresholds.']"
Build a Streamlit App With LangChain and Amazon Bedrock,https://dzone.com/articles/build-a-streamlit-app-with-langchain-and-amazon-be,"['It’s one thing to build powerful machine-learning models and another thing to be able to make them useful. A big part of it is to be able to build applications to expose its features to end users. Popular examples include ChatGPT, Midjourney, etc.', 'Streamlit is an open-source Python library that makes it easy to build web applications for machine learning and data science. It has a set of rich APIs for visual components, including several chat elements, making it quite convenient to build conversational agents or chatbots, especially when combined with LLMs (Large Language Models).', 'And that’s the example for this blog post as well — a Streamlit-based chatbot deployed to a Kubernetes cluster on Amazon EKS. But that’s not all!', 'We will use Streamlit with LangChain, which is a framework for developing applications powered by language models. The nice thing about LangChain is that it supports many platforms and LLMs, including Amazon Bedrock (which will be used for our application).', 'A key part of chat applications is the ability to refer to historical conversation(s) — at least within a certain time frame (window). In LangChain, this is referred to as Memory. Just like LLMs, you can plug-in different systems to work as the memory component of a LangChain application. This includes Redis, which is a great choice for this use case since it’s a high-performance in-memory database with flexible data structures. Redis is already a preferred choice for real-time applications (including chat) combined with Pub/Sub and WebSocket. This application will use Amazon ElastiCache Serverless for Redis, an option that simplifies cache management and scales instantly. This was announced at re:Invent 2023, so let’s explore while it’s still fresh!', 'To be honest, the application can be deployed on other compute options such as Amazon ECS, but I figured since it needs to invoke Amazon Bedrock, it’s a good opportunity to also cover how to use EKS Pod Identity (also announced at re:Invent 2023!!)', 'GitHub repository for the app.', 'Here is a simplified, high-level diagram:', '', 'Let’s go!!', 'Clone the GitHub repository:', '', 'Create an ECR repository:', '', 'Create the Docker image and push it to ECR:', '', 'Update the app.yaml file:', 'Deploy the application:', '', 'To check logs: kubectl logs -f -l=app=streamlit-chat', 'To access the application:', '', 'Navigate to http://localhost:8080 using your browser and start chatting! The application uses the Anthropic Claude model on Amazon Bedrock as the LLM and Elasticache Serverless instance to persist the chat messages exchanged during a particular session.', 'To better understand what’s going on, you can use redis-cli to access the Elasticache Redis instance from EC2 (or Cloud9) and introspect the data structure used by LangChain for storing chat history.', 'keys *', 'Don’t run keys * in a production Redis instance — this is just for demonstration purposes.', 'You should see a key similar to this — ""message_store:d5f8c546-71cd-4c26-bafb-73af13a764a5"" (the name will differ in your case).', ""Check it’s type: type message_store:d5f8c546-71cd-4c26-bafb-73af13a764a5 - you will notice that it's a Redis List."", 'To check the list contents, use the LRANGE command:', '', 'You should see a similar output:', '', 'Basically, the Redis memory component for LangChain persists the messages as a List and passes its contents as additional context with every message.', 'To be completely honest, I am not a Python developer (I mostly use Go, Java, or sometimes Rust), but I found Streamlit relatively easy to start with, except for some of the session-related nuances.', 'I figured out that for each conversation, the entire Streamlit app is executed (this was a little unexpected coming from a backend dev background). That’s when I moved the chat ID (kind of unique session ID for each conversation) to the Streamlit session state, and things worked.', 'This is also used as part of the name of the Redis List that stores the conversation (message_store:<session_id>) — each Redis List is mapped to the Streamlist session. I also found the Streamlit component-based approach to be quite intuitive and pretty extensive as well.', 'I was wondering if there are similar solutions in Go. If you know of something, do let me know.', 'Happy building!']"
PostgresML: Extension That Turns PostgreSQL Into a Platform for AI Apps,https://dzone.com/articles/postgresml-extension-that-turns-postgresql-into-a,"[""PostgresML is an extension of the PostgreSQL ecosystem that allows the training, fine-tuning, and use of various machine learning and large language models within the database. This extension turns PostgreSQL into a complete MLOps platform, supporting various natural language processing tasks and expanding Postgres's capabilities as a vector database."", 'The extension complements pgvector, another foundational extension for apps wishing to use Postgres as a vector database for AI use cases. With pgvector, applications can easily store and work with embeddings generated by large language models (LLMs). PostgresML takes it further by enabling the training and execution of models within the database.', ""Let's look at the PostgresML extension in action by using PostgreSQL for language translation tasks and user sentiment analysis."", 'The easiest way to start with PostgresML is by deploying a database instance with the pre-installed extension in Docker.', 'Use the following command to launch PostgreSQL with PostgresML in a container and open a database session with the psql tool:', '', 'Once the container has started and the psql session is open, check that the pgml extension (short for PostgresML) is on the extensions list:', '', ""Finally, if you run the \\d command, you'll see a list of database objects used internally by PostgresML."", '', 'PostgresML integrates with Hugging Face Transformers to enable the latest natural language processing (NLP) models in PostgreSQL. Hugging Face features thousands of pre-trained models that can be used for tasks like sentiment analysis, text classification, summarization, translation, question answering, and more.', 'For instance, suppose you store a product catalog in PostgreSQL, with all the product descriptions in English. Now, you need to display these descriptions in French for customers visiting your e-commerce website from France.', ""What if someone gets interested in Apple's AirTag? PostgresML can facilitate the translation from English to French using one of the translation transformers:"", '', 'If the e-commerce website also caters to Spanish-speaking countries, then product descriptions can be translated into Spanish using a different model:', '', 'Overall, PostgresML can improve user experience by returning text that has already been translated back to the application layer.', 'What about engaging in more sophisticated ML and AI-related tasks with PostgresML? One such task is the sentiment analysis of data being inserted or stored in the database.', 'Imagine that customers of the e-commerce website can share their feedback on the products. PostgresML can assist in monitoring customer sentiment about specific products and proactively responding to various concerns and complaints.', 'For example, a customer purchased a headset and shared feedback that PostgresML classified as negative:', '', 'A company representative reached out to the customer promptly and helped to solve the problem. As a result, the customer shared follow-up feedback that was classified as positive.', '', ""Just like with the translation tasks, you can utilize thousands of other models from Hugging Face for sentiment analysis and other text classification tasks. For instance, here's how you can switch to the RoBERTa model, which was trained on approximately 40,000 English posts on X (Twitter):"", '', 'The RoBERTa model has also accurately classified the sentiment of the comments, allowing the e-commerce company to address user concerns and complaints promptly as soon as negative feedback gets into PostgreSQL.', ""As a vector database, Postgres isn't limited to storing and querying embeddings. With the PostgresML extension, Postgres can be transformed into a computational platform for various AI and ML tasks."", 'Discover more about PostgresML and PostgreSQL as a vector database in the following hands-on practical guides:', '']"
Cloud Computing Trends For 2024,https://dzone.com/articles/cloud-computing-trends-for-2024,"['As we approach 2024, the cloud computing landscape is on the cusp of significant changes. In this article, I explore my predictions for the future of cloud computing, highlighting the integration of Generative AI Fabric, its application in enterprises, the advent of quantum computing with specialized chips, the merging of Generative AI with edge computing, and the emergence of sustainable, self-optimizing cloud environments.', 'The Generative AI Fabric is set to become a crucial architectural element in cloud computing, functioning as a middleware layer. This fabric will facilitate the operation of Large Language Models (LLMs) and other AI tools, serving as a bridge between the technological capabilities of AI and the strategic business needs of enterprises. The integration of Generative AI Fabric into cloud platforms will signify a shift towards more adaptable, efficient, and intelligent cloud environments, capable of handling sophisticated AI operations with ease.', 'Generative AI will play a pivotal role in enterprise operations by 2024. Cloud providers will enable easier integration of these AI models, particularly in coding and proprietary data management. This trend includes the deployment of AI code pilots that directly enhance enterprise code bases, improving development efficiency and accuracy.\xa0', 'A part from enhancing enterprise code bases, another significant trend in the integration of Generative AI in enterprises is the incorporation of proprietary data with Generative AI services. Enterprises are increasingly leveraging their unique datasets in combination with advanced AI services, including those at the edge, to unlock new insights and capabilities. This integration allows for more tailored AI solutions that are finely tuned to the specific needs and challenges of each business. It enables enterprises to gain a competitive edge by leveraging their proprietary data in more innovative and efficient ways.\xa0', 'The integration of Generative AI in enterprises will also be mindful of data security and privacy, ensuring a responsible yet revolutionary approach to software development, data management, and analytics.', 'Quantum computing will emerge as a game-changing addition to cloud computing in 2024. The integration of specialized quantum chips within cloud platforms will provide unparalleled computational power. These chips will enable businesses to perform complex simulations and solve problems across various sectors, such as pharmaceuticals and environmental science. Quantum computing in cloud services will redefine the boundaries of computational capabilities, offering innovative solutions to challenging problems.', 'An exciting development in this area is the potential introduction of Generative AI copilots for quantum computing. These AI copilots could play a crucial role in both educational and practical applications of quantum computing. For educational purposes, they could demystify quantum computing concepts, making them more accessible to students and professionals looking to venture into this field. The AI copilots could break down complex quantum theories into simpler, more digestible content, enhancing learning experiences.', 'In practical applications, Generative AI copilots could assist in the implementation of quantum computing solutions. They could provide guidance on best practices, help optimize quantum algorithms, and even suggest innovative approaches to leveraging quantum computing in various industries. This assistance would be invaluable for organizations that are new to quantum computing, helping them integrate this technology into their operations more effectively and efficiently.', 'The integration of Generative AI with edge computing is expected to make significant strides in 2024. This synergy is set to enhance the capabilities of edge computing, especially in areas of real-time data processing and AI-driven decision-making. By bringing Generative AI capabilities closer to the data source, edge computing will enable faster and more efficient processing, which is crucial for a variety of applications.', 'One of the key benefits of this integration is improved data privacy. By processing data locally on edge devices, rather than transmitting it to centralized cloud servers, the risk of data breaches and unauthorized access is greatly reduced. This localized processing is particularly important for sensitive data in sectors like healthcare, finance, and personal data services.', 'In addition to IoT and real-time analytics, other use cases include smart city management, personalized healthcare monitoring, and enhanced retail experiences. I have covered the future of retail with Generative AI in my earlier blog', 'Sustainable cloud computing will become a pronounced trend in 2024. Self-optimizing cloud environments focusing on energy efficiency and reduced environmental impact will rise. These systems, leveraging AI and automation, will dynamically manage resources, leading to more eco-friendly and cost-effective cloud solutions. This trend towards sustainable cloud computing reflects a global shift towards environmental responsibility.', 'As 2024 approaches, the cloud computing landscape is set to undergo a series of transformative changes. The development of Generative AI Fabric as a middleware layer, its integration into enterprise environments, the emergence of quantum computing with specialized chips, the fusion of Generative AI with edge computing, and the rise of sustainable, self-optimizing cloud infrastructures are trends that I foresee shaping the future of cloud computing. These advancements promise to bring new efficiencies, capabilities, and opportunities, underscoring the importance of staying informed and adaptable in this evolving domain.']"
Use pgvector With PostgreSQL To Improve LLM Accuracy and Performance,https://dzone.com/articles/use-pgvector-with-postgresql-to-improve-llm-accura,"['If you’re not yet familiar with the open-source pgvector extension for PostgreSQL, now’s the time to do so. The tool is extremely helpful for searching text data fast without needing a specialized database to store embeddings.', 'Embeddings represent word similarity and are stored as vectors (a list of numbers). For example, the words “tree” and “bush” are related more closely than “tree” and “automobile.” The open-source pgvector tool makes it possible to search for closely related vectors and find text with the same semantic meaning. This is a major advance for text-based data, and an especially valuable tool for building Large Language Models (LLMs)... and who isn’t right now?', 'By turning PostgreSQL into a high-performance vector store with distance-based embedding search capabilities, pgvector allows users to explore vast textual data easily. This also enables exact nearest neighbor search and approximate nearest neighbor search using L2 (or Euclidian) distance, inner product, and cosine distance. Cosine distance is recommended by OpenAI for capturing semantic similarities efficiently.', 'Embeddings can play a valuable role in the Retrieval Augmented Generation (RAG) process, which is used to fine-tune LLMs on new knowledge. The process includes retrieving relevant information from an external source, transforming it into an LLM digestible format, and then feeding it to the LLM to generate text output.\xa0', 'Let’s put an example to it. Searching documentation for answers to technical problems is something I’d bet anyone here has wasted countless hours on. For this example below, using documentation as the source, you can generate embeddings to store in PostgreSQL. When a user queries that documentation, the embeddings make it possible to represent the words in a query as vector numbers, perform a similarity search, and retrieve relevant pieces of the documentation from the database. The user’s query and retrieved documentation are both passed to the LLM, which accurately delivers relevant documentation and sources that answer the query.\xa0', 'We tested out pgvector and embeddings using our own documentation at Instaclustr. Here are some example user search phrases to demonstrate how embeddings will plot them relative to one another:\xa0', 'Embeddings plot the first two phases nearest each other, even though they include none of the same words.', '', 'Each LLM has a context window: the number of tokens it can process at once. This can be a challenge, in that models with a limited context window can falter with large inputs, but models trained with large context windows (100,000 tokens, or enough to use a full book in a prompt) suffer from latency and must store that full context in memory. The goal is to use the smallest possible context window that generates useful answers. Embeddings help by making it possible to provide the LLM with only data recognized as relevant so that even an LLM with a tight context window isn’t overwhelmed.\xa0', 'The model that generates embeddings — OpenAI’s text-embedding-ada-002 — has a context window of its own. That makes it essential to break documentation into chunks so this embedding model can digest more easily.\xa0', 'The LangChain Python framework offers a solution. An LLM able to answer documentation queries needs these tasks completed first:', 'This process yields the semantic index of documentation we’re after.\xa0', 'Now consider this sample workflow for a user query (sticking with our documentation as the example tested). First, a user submits the question: “How do I create a Redis cluster using Terraform?” OpenAI’s embeddings API calculates the question’s embeddings. The system then queries the semantic index in PostgreSQL using cosine similarity, asking for the original content closest to the embeddings of the user’s question. Finally, the system grabs the original content returned in the vector search, concatenates it together, and includes it in a specially crafted prompt with the user’s original question.\xa0', 'Now let’s see how we put pgvector into action. First, we enabled the pgvector extension in our PostgreSQL database, and created a table for storing all documents and their embeddings:\xa0', '', 'The following Python\xa0code scrapes the documentation, uses Beautiful Soup to extract main text parts such as title and content, and stores them and the URL in the PostgreSQL table:', '', 'Next, we loaded the documentation pages from the database, divided them into chunks, and created and stored the crucial embeddings.', '', 'Lastly, the retriever found the correct information to answer a given query. In our test example, we searched our documentation to learn how to sign up for an account:', '', 'Using Streamlit, a powerful tool for building interactive Python interfaces, we built this interface to test the system and view the successful query results:', '', 'Harnessing PostgreSQL and the open-source pgvector project empowers users to leverage natural language queries to answer questions immediately, with no need to comb through irrelevant data. The result: super accurate, performant, and efficient LLMs, groundbreaking textual capabilities, and meaningful time saved!']"
Few-shot learning AI accurately 'senses' home appliances,https://techxplore.com/news/2022-11-few-shot-ai-accurately-home-appliances.html,"[""NIALM (Non-Intrusive Appliance Load Monitoring) can sense appliances using electrical power. NIALM is used in homes and small buildings. For this, NIALM can require hundreds of labeled power signal images from each appliance type to train on. But there is a much faster and more cost-effective approach than traditional machine learning. Researchers from the University of Johannesburg deployed Few Shot Learning (FSL) for NIALM. Classical FSL needs only 10 labeled, classified images to recognize appliances with very high accuracy. They adapted the process so the AI (artificial intelligence neural network) can choose the best training images by itself. This makes the training process even faster. When they had also tuned some hyper parameters, as few as seven test images were sufficient for FSL to recognize appliances with 97.83 accuracy. One training image (one-shot learning) resulted in accuracy of 88.17 to 91.343 depending on the number of classes during testing. Their research is published in Computational Intelligence and Neuroscience. FSL: When machines learn to lear Teaching an AI to recognize an appliance's power signal usually needs a lot of data. Typically, an AI will need hundreds of images labeled by humans, to recognize each appliance type, despite different capacities and operating status. An example of two operating states for one appliance are the washing and spinning cycles for a washing machine. All that training data for an AI has to be created and labeled by humans, which gets slow and expensive very quickly. But there is another way for an AI to learn, that needs very little labeled data indeed. As few as 10 labeled training images can be enough for image classification with extreme accuracy. As example, say an AI is trained this way with ten images each of elephants, tigers and bears. When the AI is tested with an unlabeled image of a large male lion, the AI should recognize that the lion is similar to a tiger, but not the same. Then the AI should decide by itself to create a new object class for the lion. Also, when that AI is confronted with an unlabeled image of a lion cub, it should be able put the cub in the same class as the male lion. This type of AI machine learning (ML) algorithm is called few-shot learning (FSL). It is a form of Meta-Learning, or learning-to-learn. FSL already powers gigantic language models for dominant global technology companies. Computer vision systems that check passports against travelers' faces at some airports also use FSL. The parts of a cat Few-shot learning is really about training an AI neural network with some data, even incomplete data about an object class, says Prof Yanxia Sun from the Department of Electrical and Electronic Engineering Science, at UJ. Sun is the lead author on the study. While we train a neural network with training images, the AI learns the characteristics of each animal or object by itself. In the tiger-versus-lion example, a FSL AI learns about whiskers, cat eyes, fur, and cat tails from tiger images. It has never seen a lion before. But when the AI is tested with an image of a lion it should recognize a lion as an object class similar to, but not the same, as a tiger. NIALM: One power consumption meter for many appliances NIALM is used in small commercial buildings or in homes, to measure how much power is consumed by each appliance or piece of equipment. NIALM uses power disaggregation to pull apart the combined power consumption signal of lots of appliances switched on at the same time, on the same electrical phase. NIALM uses only one measuring device. This is far easier and quicker than having to physically connect a power meter to each appliance in turn Home smart meters in some countries store the data about the power consumption of each appliance and send that to power utilities. In other countries, the smart meters make the energy consumption data available to homeowners also. Power consumption signal to digital image In this study, the researchers trained their FSL AI on NIALM images of power load signals from a variety of home appliances. They got the power consumption signals by plugging a power analyser (Tektronix PA1000) and each appliance in turn into a multiplug power extension. They then turned the power analyzer on. The appliance was then turned on and off, while the power analyzer recorded the power consumption over time. For the laptop and desktop computer, entire boot-up sequences were recorded. Researchers from the University of Johannesburg tested a Few Shot Learning (FSL) algorithm for recognizing home appliance power signals. As the number of testing images increases, the average accuracy increases from a minimum of 91.343% to a maximum of 97.83%. Credit: Liston Matindife and Therese van Wyk. Data from study in Computational Intelligence and Neuroscience. (DOI: 10.1155/2022/2142935 ) creativecommons.org/licenses/by/4.0/ The power analyzer converted the analog power consumption signal of the appliance into digital data. That data was then converted into Gramian angular summation fields (GASF), which look like brightly colored tiles. The color GASF images of 400 X 400 pixels were then converted into grayscale and reduced in size to 28 X 28 pixels. This reduced the complexity of the algorithms and used less computation resources. Classical FSL is a two-stage process—training and testing. In this study, the researchers added a tweak to speed up the selection of very suitable data for training, which would also speed up the training process itself. We increased the accuracy of the FSL algorithm by implementing an initial assessment of the ease or suitability of our data for metric learning. We call this a similarity test, says Dr. Liston Matindife, the first author of the study. During the study, Matindife was a Ph.D. candidate at the University of Johannesburg. He currently teaches in Zimbabwe, at the National University of Science and Technology. If a GASF image fails the similarity test, it means the data needs more pre-processing, especially at time-series or waveform format, before converting into a GASF image. Images that passed the similarity test allowed our model to learn faster, adds Matindife. To train the FSL AI, the researchers fed it GASF images of 10 of the 14 classes of appliances in the study. They used 10 images per appliance class: A laptop computer switching on, a laptop computer running MSWord, a desktop computer, a refrigerator, a two-plate stove, and a variety of low-energy consumption lamps. Then they tested the FSL AI to see how well it had learnt to recognize, or classify, appliances; and learnt to create new classes for appliances it hasn't seen before. In the case of the laptop computer test images, the FSL algorithm was 97.83% precise in classifying (recognizing) the test images as coming from a laptop computer's power consumption signal, but in a new operational state. This new operational state was displaying video, not booting up or running MSWord as in the training images. The FSL AI achieved this precision with only seven testing images for booting up and another seven for running MS Word. From few-shot to one-shot learning The researchers also varied the number of training images, and then measured the classification accuracy of the algorithm. The testing shows that as the number of testing images per class increases, the average accuracy increases from a minimum 91.343% to a maximum 97.83%. This shows that FSL can be applied in NIALM recognition. NIALM algorithm development is data extensive. For appliances that have varying activation periods we would generate a different number of dataset images per appliance. This imbalance in the number of training images for different appliances would normally affect the training algorithm, says Matindife. Our algorithm reduces the need for expensive acquisition of appliance specific data. The use of the prototypical network FSL algorithm allows for inputting appliance datasets that have different number of sample images without difficulty, he adds. This study shows that it is possible to achieve an average accuracy of 90% with just one training image per class, when using Siamese and prototypical FSL algorithms based on computer vision applied to GASF graphs. When one training image results in sufficient accuracy, it is called one-shot learning. One-shot learning could solve a huge challenge in NIALM—the large number of training images required, says Matindife. that AI sees the power signal from a domestic refrigerator with compressor motor problems from brand C and capacity D, it should be able to flag that the second appliance has a power signal problem, she says.""]"
A new model to enable multi-object tracking in unmanned aerial systems,https://techxplore.com/news/2021-10-enable-multi-object-tracking-unmanned-aerial.html,"[""To efficiently navigate their surrounding environments and complete missions, unmanned aerial systems (UASs) should be able to detect multiple objects in their surroundings and track their movements over time. So far, however, enabling multi-object tracking in unmanned aerial vehicles has proved to be fairly challenging. Researchers at Lockheed Martin AI Center have recently developed a new deep learning technique that could allow UASs to track multiple objects in their surroundings. Their technique, presented in a paper pre-published on arXiv, could aid the development of better performing and more responsive autonomous flying systems. We present a robust object tracking architecture aimed to accommodate for the noise in real-time situations, the researchers wrote in their paper. We propose a kinematic prediction model, called deep extended Kalman filter (DeepEKF), in which a sequence-to-sequence architecture is used to predict entity trajectories in latent space. The kinematic prediction model created by Wanlin Xie, Jaime Ide and their colleagues at Lockheed Martin AI Center essentially uses an acquired image embedding and a computational attention mechanism to weigh the 'importance' of different parts of an image for predicting changes and future states. Subsequently, the model utilizes similarity measures to calculate distances between objects, by analyzing images using a convolutional neural network (CNN) encoder, pre-trained using Siamese neural networks. A siamese neural network is an AI technique in which two identical neural networks generate feature vectors for each individual data input and compare these vectors. These approaches can be particularly useful in situations where researchers are trying to detect anomalies or differences in images, as well as for face and object recognition applications. The researchers evaluated their deep learning technique using annotated video footage collected by a camera integrated on a fixed-wing UAS. These labeled video sequences contained a series of moving objects, including people and vehicles. We wanted to precisely diagnose how well our model can accurately and consistently keep track of distinct object entities over continuous periods of time, the researchers wrote in their paper. We look at several performance measures including absence prediction, prediction recall plots, longevity of tracking, etc. A Kalman filter (KF) is an algorithm that can estimate some unknown variables, when it is fed a series of measurements collected over time. The multi-object tracking approach proposed by the researchers is a more advanced version of a KF, which also integrates deep learning techniques. In initial evaluations, the DeepEKF architecture developed by Xie, Ide and their colleagues achieved remarkable results, outperforming standard KF algorithms for multi-object tracking. In the future, their framework could thus be used to enhance the capabilities of a variety of UASs. Although we report proof of concept results, further training of the DeepEKF as well as of the Siamese networks are necessary as we collect more data, the researchers wrote in their paper. In particular, we plan to add a more extensive evaluation for the long-term tracking (re-identification) component. Another promising venue is to dynamically combine the different kinematic and visual scores within the similarity fuser component given the environment and track states.""]"
"Intel, Google, UC Berkeley AI team trains robot to do sutures",https://techxplore.com/news/2020-06-intel-google-uc-berekely-ai.html,"[""The next time you go to a hospital for surgery, the surgeons assistant may be a robot. In a collaboration between Google Brain, Intel Corporation and the University of California, Berkeley, researchers have trained robots to mimic surgical procedures through the use of instructional videos. UC Berkeley professors have previously used YouTube videos as a guide for robots to learn various motions such as jumping or dancing, while Google has trained robots to understand depth and motion. The team applied that knowledge to their latest project, Motion2Vec, in which videos of actual surgical procedures are used for instruction. In a recently released research paper, researchers outline how they used YouTube videos to train a two-armed da Vinci robot to insert needles and perform sutures on a cloth device. The medical team relied on Siamese networks, a deep-learning setup that incorporates two or more networks sharing the same data. The system is optimal for comparing and assessing relationships between datasets. Such networks have been used in the past for facial detection, signature verification and language detection. Ken Goldberg, a physician who heads the UC Berkeley laboratory, explains YouTube is a rich source of instructional materials for this deep-learning project. YouTube gets 500 hours of new material every minute. Its an incredible repository, he said. Any human can watch almost any one of those videos and make sense of it, but a robot currently cannot—they just see it as a stream of pixels. So the goal of this work is to try and make sense of those pixels. That is to look at the video, analyze it, and… be able to segment the videos into meaningful sequences. For the suture task, the team needed only 78 instructional medical videos to train its AI engine to perform the procedure. They claim a success rate of 85 percent. This means that robots can eventually undertake some of the more basic, repetitive tasks in surgical procedures and allow surgeons to focus their time and energy on the more exacting steps. Will robots replace surgeons soon? We're not there yet Goldberg said. But what we're moving towards is the ability for a surgeon, who would be watching the system, to indicate where they want a row of sutures, convey that they want six overhand sutures, Goldberg said. Then the robot would essentially start doing that and the surgeon would... be able to relax a little bit so that they could then be more rested and able to focus on more complex or nuanced parts of the surgery. Machine learning has contributed much to biotechnology indecent years. The ability of AI to rapidly process huge volumes of data has yielded progress in detecting lung cancer and stroke risk based on CAT scans, calculated risk of heart disease and cardiac arrest based on EKG and MRI imagery, classified skin lesions from photos and detected signs of diabetic distress in eye images. And here in the middle of a pandemic, AI is helping scientists find drugs that may curb the spread of COVID-19 and eventually find a cure and a vaccine.""]"
SPFCNN-Miner: A new classifier to tackle class-unbalanced data,https://techxplore.com/news/2019-06-spfcnn-miner-tackle-class-unbalanced.html,"[""Researchers at Chongqing University in China have recently developed a cost-sensitive meta-learning classifier that can be used when the training data available is high-dimensional or limited. Their classifier, called SPFCNN-Miner, was presented in a paper published in Elsevier's Future Generation Computer Systems. Although machine learning classifiers have proved to be effective in a variety of tasks, to achieve optimal results, they often require a vast amount of training data. When data is high-dimensional, limited or unbalanced, most classification methods are unable to achieve a satisfying performance. In their study, the team of researchers at Chongqing University set out to better understand these data-related challenges and develop a classifier that can overcome them. We used Siamese networks that are suitable for few-shot learning where a little data is available to learn high-dimensional and limited data, and apply the idea of combining 'shallow' and 'deep' approaches to design parallel Siamese networks that can better extract simple or complex features from a variety of datasets, Linchang Zhao, one of the researchers who carried out the study, told TechXplore. The main objectives of our study were to solve the data class-imbalanced problem and get the best possible classification results on such datasets. Zhao and his colleagues developed a Siamese parallel fully-connected neural network (SPFCNN) and applied it to problems with class-unbalanced data distributions. To transform their cost-insensitive SPFCNN into a cost-sensitive approach, they used a technique called 'cost-sensitive learning. First, the researchers divided the majority group in a dataset based on inner-product transformed features. This ensured that the size of each sub-group in a majority group was close to that of the minority group. In addition, they structured some sub-ensembles using the minority group vs. each partition obtained. Next, we applied n SPFCNN-miners to all sub-ensembles, each sample point xjcan be expressed by its corresponding measures(dj1,…, djn), each sub-classifier can be transformed into a measure of contrastive loss function by fitting the SPFCNN, Zhao explained. Finally, n SPFCNN-miners were integrated as a final classifier according to the values of contrastive function. The approach devised by Zhao and his colleagues has numerous advantages that set it apart from other classifiers. First, their Meta-Learner Function (MLF) can be used to partition the majority group in a dataset based on the inner-product transformed features, which results in the transformed data containing information related to distances and angles between items in the minority and majority groups. The angles between the majority group and the minority group can be viewed as the expression of related locations and then represent the related direction of the majority group to the minority group,Zhao explained. A further advantage of the new SPFCNN-Miner classifier is that, like other Siamese networks, it can effectively extract the highest-level features from a small amount of samples for few-shot learning. Moreover, parallel Siamese networks are designed to adaptively learn simple or complex features from different dimensions of data attributes. Zhao and his colleagues evaluated their approach in a series of computational tests, using both cost-insensitive and cost-sensitive versions of the SPFCNN classifier. They found that the cost-sensitive approach outperformed all of the classifiers they compared it with. The experimental results show that our SPFCNN is a competitive approach and is able to improve the classification performance more significantly compared with the benchmarked approaches, Zhao said.We found that the performance of our model did not improve as the sample size increased, but was greatly affected by the imbalance rate. The performance obtained by incorporating the cost-sensitive learning into our model is more stable. The study carried out by Zhao and his colleagues introduces a new method that could be used by researchers to enhance the performance of classifiers when data is limited or unbalanced. In addition, their findings suggest that balancing the number of positive and negative samples can be more effective than generating a larger number of artificial samples. For instance, their approach can integrate different misclassification costs as it completes a classification task, which makes it more robust than other techniques used to address issues unbalanced data-related issues. In the future, we plan to use techniques such as random walk matrices, circulant weight sharing and Huffman coding to compress our model, and the loosely-connected technology or paralleled pruning-quantization method will be used to lightweight the proposed SPFCNN model, Zhao said.""]"
Researchers trained neural networks to be fashion designers (sort of),https://techxplore.com/news/2017-11-neural-networks-fashion.html,"[""Researchers from the University of California San Diego and Adobe Research have demonstrated how artificial intelligence and neural networks could one day create custom apparel designs to help retailers and apparel makers sell clothing to consumers based on what they learned from a buyer's preferences. We show that our model can be used generatively, i.e., given a user and a product category, we can generate new images (in this case clothing items) that are most consistent with the user's personal taste, said first-author and computer science Ph.D. student Wang-Cheng Kang. This represents a first step toward building systems that go beyond recommending existing items from a product corpus, to suggesting styles and helping to design new products. Their findings were published in early November on ArXiv in a paper titled Visually-Aware Fashion Recommendation and Design with Generative Image Models. Computer Science and Engineering professor Julian McAuley and his second-year Ph.D. student, Wang-Cheng Kang, teamed on the research with industry experts Chen Fang and Zhaowen Wang from Adobe Research. This suggests a new type of recommendation approach that can be used for recommendation, production and design, McAuley and his colleagues write. These frameworks can lead to richer forms of recommendation, where content recommendation and content generation are more closely linked.The project aimed to test how well tools from artificial intelligence and machine learning can help the fashion industry and consumers—particularly those among the growing cohort of shoppers eager to buy clothing on the Internet. While there are many algorithms and tools to help online retailers recommend designs to potential buyers, the UC San Diego-Adobe Research team went a giant step further. They wanted to see if it would be possible to crunch preference and other data not only to make recommendations, but potentially to enable computers to produce new clothing designs that would have an edge because they reflect a consumer's individual preferences. Each row is a separate retrieval/optimization process for a given user and product category. At left are real images; at right, synthetic ones. The values are preference scores for each image. Credit: University of California San Diego Initially, the researchers focused on devising a system to create better recommendations, particularly in the case of 'visual' recommendations, where consumers can be swayed by how the product looks, as in the case of fashion apparel or artworks). Building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved, according to the new paper. Researchers went on to note that visual recommendation can be made more accurate by incorporating visual signals directly into the recommendation objective, using off-the-shelf feature representations derived from deep networks. The team demonstrated that recommendation performance can be significantly improved by learning 'fashion aware' image representations directly, by training the image representation (from the pixel level) and the recommender system jointly. The paper grew out of recent work using Siamese Convolutional Neural Networks (Siamese-CNNs), so-called because they are a class of neural network architectures containing two or more identical subnetworks. (Siamese-CNNs are popularly used to find similarity or relationships between two comparable items.) For the fashion project, the researchers trained the Siamese-CNN to learn and classify a user's preferences for certain items. From there, they used a neural networking framework called Generative Adversarial Network (GAN) to learn the distribution of fashion images and generate novel fashion items that maximize users' preferences. GANs train two networks on one set of data, and they have been particularly well-suited to generate realistic images. The resulting system can suggest items to buy from existing designs, but it can also be used to modify existing items, or to generate new designs tailored to a specific individual's preferences (based on 'big data ' about prior purchases, surveys, etc.). The use of AI in the fashion industry is still in its infancy, but two of the world's largest online retailers—Amazon and China's Alibaba—are already working with AI tools, including GANs. As for the UC San Diego-Adobe Research project, the quality of the algorithmically-designed new clothes remains rudimentary at best. As CSE's McAuley told a writer from MIT Technology Review, you'd have to read the tea leaves a little bit if you want to call that style or not. But, he warned, bringing neural networking to the fashion world is in its infancy.""]"